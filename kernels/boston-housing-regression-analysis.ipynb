{"cells":[{"metadata":{"id":"tixMQpz56WmI","colab_type":"text"},"cell_type":"markdown","source":"This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston MA\nAs mentioned about [The Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). There are 14 attributes in each case of the dataset. They are:\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in $1000's\n\nVariable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000); Censoring is suggested by the fact that the highest median price of exactly $50,000 is reported in 16 cases, while 15 cases have prices between $40,000 and $50,000, with prices rounded to the nearest hundred.\n\nOur goal is to select the valiables which predicts the MEDV best, also to suggest a machine learning model to predict MEDV"},{"metadata":{"id":"Ww6ipgJes14L","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"id":"gYGRqrjTtBCA","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/boston-housing/Boston Housing.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"L8D1YvBxtXRJ","colab_type":"code","outputId":"35828b05-8036-4f0d-ab90-c75c6684f28a","colab":{"base_uri":"https://localhost:8080/","height":195},"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"V7Xwzdq8tIOS","colab_type":"code","outputId":"cde191d7-e3e0-4e45-c2a8-acecf129b13a","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"iA5ZyeU5tqvW","colab_type":"text"},"cell_type":"markdown","source":"As per the above observation we have checked the count, mean, percentile(etc.)\nNow if we will see the data \n1. ZN column has 0 values in 25% - 50% and it is skewed its gaining results at 75% and above. As ZN is proportion of residential land zoned for lots over 25,000 sq.ft hence we can understand that it is conditional data\n2. Chas column has 0 values in 25%, 50%, 75%. As CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise) and it is a categorized data (0,1).\nSo we have concluded that both ZN & Chas columns are skewed and can impact MEDV so we have to remove ZN & Chas, lets drop it."},{"metadata":{"id":"i75PUQ0Rv_0A","colab_type":"text"},"cell_type":"markdown","source":"#Removing ZN & Chas\n  "},{"metadata":{"id":"BszxaYAfwq4P","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df = df.drop([\"ZN\", \"Chas\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"qA2CTNgDwu0p","colab_type":"code","outputId":"39418786-fa29-489a-c055-be4c7e0844bd","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"3EVeaKwwwzVb","colab_type":"code","outputId":"963f4405-a685-4f24-fcc9-becbb3340890","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#checking columns / rows\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Km9FZEx8w_eE","colab_type":"code","outputId":"f521d1f2-362a-4424-d624-487c06598120","colab":{"base_uri":"https://localhost:8080/","height":235},"trusted":true},"cell_type":"code","source":"#lets see if we have any null values in our data\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"hdvvq-WDxN2Y","colab_type":"code","outputId":"078460eb-90d3-4be0-a4d3-dfa9c4b833fd","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"#moving to our next step to treat Outliers\n#lets visualize the data through box plot\nfor i in df.columns:\n  sns.boxplot(y=i, data=df)\n  plt.tight_layout(pad=0.4)\n  plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ud_zvnr81WN4","colab_type":"text"},"cell_type":"markdown","source":"From above boxplot we can see that Columns CRIM, RM, DIS, PTRATIO, B, LSTAT and MEDV have outliers.\n"},{"metadata":{"id":"LTa-56YlYNlb","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"OmbOKJGA1gK7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Lets check the outliers using for loop and removing the outliers\nfor i in df2.columns:\n  df2.sort_values(by=i, ascending=True, na_position='last') #sorting is required before percentile\n  q1, q3 = np.percentile(df2[i], [25,75])\n  iqr = q3-q1\n  upper_bound = q3+(1.5 * iqr)\n  lower_bound = q1-(1.5 * iqr)\n  mean = df2[i].mean()\n  df2.loc[df2[i]< lower_bound, [i]] = mean\n  df2.loc[df2[i]> upper_bound, [i]] = mean","execution_count":null,"outputs":[]},{"metadata":{"id":"OlDf4M86sULJ","colab_type":"code","outputId":"15d02bf0-78b8-474e-c436-1f0e7f108e0a","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"0ef20152-3449-4bca-b350-6d0ad6dd86d9","id":"Vzl_GZvEtETq","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"df2.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"XpMGV8Cxcmye","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#importing dataset\nX = df2.iloc[:, :-1] #independent variable\ny = df2.iloc[:, 11] #dependent variable","execution_count":null,"outputs":[]},{"metadata":{"id":"k3q2jXq5NZTB","colab_type":"code","outputId":"6be72d7a-3d96-40f3-f6a6-52ea59bcaad2","colab":{"base_uri":"https://localhost:8080/","height":672},"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nX = df2.iloc[:, :-1] #independent variable\ny = df2.iloc[:, 11] #dependent variable\nX = sm.add_constant(X) # adding a constant\n\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X) \n\nprint_model = model.summary()\nprint(print_model)","execution_count":null,"outputs":[]},{"metadata":{"id":"M9ErIJcL8vc9","colab_type":"code","outputId":"d7a76346-a01a-408b-e39b-2e0a5d01ca27","colab":{"base_uri":"https://localhost:8080/","height":278},"trusted":true},"cell_type":"code","source":"#lets make the correlation matrix\ncorr_data = df2.corr()\ncorr_data.style.background_gradient(cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"hWHd5pbl-90O","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#as per the above correlation matrix we can see that Tax & RAD are highly correlated,\n#as per the observation of data Rad is more important variable in predicting the Medv so I am dropping Tax here\ndf2 = df2.drop([\"Tax\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZEkGX4tUBVOM","colab_type":"code","outputId":"00591233-67e7-4c70-a069-48c78ebf6af4","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Pa32Di1wBVDR","colab_type":"code","outputId":"2cb7378c-90b6-4a20-ed91-0fc0c3813029","colab":{"base_uri":"https://localhost:8080/","height":258},"trusted":true},"cell_type":"code","source":"#lets make the correlation matrix\ncorr_data = df2.corr()\ncorr_data.style.background_gradient(cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"UrJNKA9MBU5J","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#now after dropping TAX we have noticed that CRIM and RAD are highly correlated and as per the observation of data it is suggested to drop RAD rather than CRIM in respect to MEDV\ndf2 = df2.drop([\"Rad\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"0CmcsTnqBUrF","colab_type":"code","outputId":"ec7d8322-3522-464c-a6b4-04f4bb620518","colab":{"base_uri":"https://localhost:8080/","height":238},"trusted":true},"cell_type":"code","source":"#lets make the correlation matrix again\ncorr_data = df2.corr()\ncorr_data.style.background_gradient(cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"BrmSmePaFqFJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#now after dropping RAD we have noticed that NOX and INDUS are highly correlated and as per the observation of data it is suggested to drop INDUS rather than NOX in respect to MEDV\ndf2 = df2.drop([\"Nox\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-uWMgO47GE0j","colab_type":"code","outputId":"0f488095-c961-4c3e-8c1e-5367f4839074","colab":{"base_uri":"https://localhost:8080/","height":218},"trusted":true},"cell_type":"code","source":"#lets make the correlation matrix again\ncorr_data = df2.corr()\ncorr_data.style.background_gradient(cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Zc-CoKlkGuaP","colab_type":"code","outputId":"02b970e1-d798-455d-842d-aa6dfbffdc20","colab":{"base_uri":"https://localhost:8080/","height":168},"trusted":true},"cell_type":"code","source":"#till now we have removed the highly correlated data that might impact our predictions of MEDV. \n#Lets see the correlation of MEDV with other variables and remove the less correlated variables (using pearson method here)\nfrom scipy.stats import pearsonr\nfor i in df2.columns:\n  corr, p_val = pearsonr(df2[i], df2[\"Medv\"])\n  print (i, corr)","execution_count":null,"outputs":[]},{"metadata":{"id":"B_oyg6VwGuuT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#from above pearson methond I have concluded that B is least correlated and have least impact on MEDV so removing same\ndf2 = df2.drop([\"B\"], axis= 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"0_gDeY__Gu1M","colab_type":"code","outputId":"398ff076-724f-4f11-c986-f3764792853e","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"df2.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"srLRgQrHUZ_8","colab_type":"text"},"cell_type":"markdown","source":"Lets implement the Machine learning models\nAs we know that this is a regression a problem as we have to predict a continous (non catagorical) value."},{"metadata":{"id":"9JN2I93tiaKo","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#splitting data into train test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q2vFBkxTWd3h","colab_type":"text"},"cell_type":"markdown","source":"Linear Regression"},{"metadata":{"id":"uFRwPbcNWBKa","colab_type":"code","outputId":"31e23db8-dcbc-4a66-9b92-ac1125f61aa9","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\ny_compare = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nfrom sklearn.model_selection import cross_val_score as cvs\naccuracy = cvs(lr, X_train, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare.head() #Comparison b/w Actual & Predicted","execution_count":null,"outputs":[]},{"metadata":{"id":"Pet-iooXX123","colab_type":"text"},"cell_type":"markdown","source":"Polynomial Regression"},{"metadata":{"id":"YtoC2kTufapq","colab_type":"code","outputId":"6b061e4a-60a3-454c-befa-96d03c03b969","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"#fitting polynomial regression.....WHEN WE USE POLYNOMIAL REGRESSION WE HAVE TO FIT DATASET IN LINEAR REGRESSION FIRST\nfrom sklearn.preprocessing import PolynomialFeatures\npolyReg = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = polyReg.fit_transform(X_train)\nX_test_poly = polyReg.fit_transform(X_test)\npoly = LinearRegression()\npoly.fit(X_train_poly, y_train)\ny_pred = poly.predict(X_test_poly)\ny_compare_poly = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\naccuracy = cvs(poly, X_train_poly, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"CtpX7TvHgfdG","colab_type":"text"},"cell_type":"markdown","source":"Support Vector Regression"},{"metadata":{"id":"IKzorZorgDmK","colab_type":"code","outputId":"d687a4dd-0be2-4e98-f26e-8aee3fea37ee","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr = SVR (kernel = 'rbf', gamma = 'scale')\nsvr.fit(X_train, y_train)\ny_pred = svr.predict(X_test)\ny_compare_svr = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\naccuracy = cvs(svr, X_train, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SvHIdDjIoS3i","colab_type":"text"},"cell_type":"markdown","source":"Decission Tree"},{"metadata":{"id":"5Y4DA7r7n7d3","colab_type":"code","outputId":"b4397680-4f88-44cb-a29c-90030e7259af","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor (random_state = 0)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\ny_compare_dt = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\naccuracy = cvs(dt, X_train, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare_dt.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"vaadRyVGqHxR","colab_type":"text"},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"id":"al31uUFFpmrU","colab_type":"code","outputId":"09321596-d9ad-4916-e7f2-21b9ed4474ac","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor(n_estimators = 160, random_state = 0)\nRF.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\ny_compare_RF = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\naccuracy = cvs(RF, X_train, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare_RF.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"P8VwAqbqsBBd","colab_type":"text"},"cell_type":"markdown","source":"K-NN"},{"metadata":{"id":"LAM-JsL0rk_t","colab_type":"code","outputId":"635c5f61-718e-4741-8dee-df364ec92f8c","colab":{"base_uri":"https://localhost:8080/","height":212},"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nKNN = KNeighborsRegressor(n_neighbors = 4)\nKNN.fit(X_train, y_train)\ny_pred = KNN.predict(X_test)\ny_compare_KNN = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\naccuracy = cvs(KNN, X_train, y_train, scoring='r2', cv=5)\nprint (accuracy.mean())\ny_compare_KNN.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Im76O-1wtaRv","colab_type":"text"},"cell_type":"markdown","source":"Plotting compariasion of actual and predicted values of MEDV that we got using different machine learning models\n\n"},{"metadata":{"id":"xgju9niJs5Rq","colab_type":"code","outputId":"4dae1c03-cc89-41b7-b183-60e8cfa2a624","colab":{"base_uri":"https://localhost:8080/","height":327},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(25,4))\nax = ax.flatten()\ny_compare.head(10).plot(kind='bar', title='Linear Regression', grid='True', ax=ax[0])\ny_compare_dt.head(10).plot(kind='bar', title='Decission Tree', grid='True', ax=ax[1])\ny_compare_KNN.head(10).plot(kind='bar', title='KNN', grid='True', ax=ax[2])\ny_compare_RF.head(10).plot(kind='bar', title='Random Forest', grid='True', ax=ax[3])\ny_compare_svr.head(10).plot(kind='bar', title='SVR', grid='True', ax=ax[4])\ny_compare_poly.head(10).plot(kind='bar', title='Poly', grid='True', ax=ax[5])","execution_count":null,"outputs":[]},{"metadata":{"id":"Lva2Ntxhvbsg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"75ced5a4-6209-4ffe-9c8c-234781387727","trusted":true},"cell_type":"code","source":"print('According to R squared scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear', 'Polynomial', 'Support Vector', 'Random Forrest', 'K-Nearest Neighbour', 'Decission Tree']\nmodelRegressors = [lr, poly, svr, RF, KNN, dt]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n  if i is poly:\n    accuracy = cvs(i, X_train_poly, y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  else:\n    accuracy = cvs(i, X_train, y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"URxEui3jmt7J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"dc8c54fd-21db-420f-98e5-dd3b9e1a757c","trusted":true},"cell_type":"code","source":"print('According to Mean Absolute Error scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear', 'Polynomial', 'Support Vector', 'Random Forrest', 'K-Nearest Neighbour', 'Decission Tree']\nmodelRegressors = [lr, poly, svr, RF, KNN, dt]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n  if i is poly:\n    accuracy = cvs(i, X_train_poly, y_train, scoring='neg_mean_absolute_error', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  else:\n    accuracy = cvs(i, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"MkVzYsV1m6VE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"575a5ff4-bae1-4cca-ba54-7d39bed3aa9c","trusted":true},"cell_type":"code","source":"print('According to Mean Squared Error scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear', 'Polynomial', 'Support Vector', 'Random Forrest', 'K-Nearest Neighbour', 'Decission Tree']\nmodelRegressors = [lr, poly, svr, RF, KNN, dt]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n  if i is poly:\n    accuracy = cvs(i, X_train_poly, y_train, scoring='neg_mean_squared_error', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  else:\n    accuracy = cvs(i, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"HMK0RuBBqONt","colab_type":"text"},"cell_type":"markdown","source":"From above results of R2 MSE & MAE we found that Random Forest gives us the best results to predict MEDV.\n\nI would like to close it by mentioning an important fact, that no Data Science technique is perfect and there is always scope for imporvement."},{"metadata":{"id":"oODjc6-kq7IQ","colab_type":"text"},"cell_type":"markdown","source":"**Please comment your suggestions.**\n\n**Please upvote if this notebook is helpful.**"}],"metadata":{"colab":{"name":"Boston Housing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}