{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fraud Detection Competition"},{"metadata":{},"cell_type":"markdown","source":"IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge."},{"metadata":{},"cell_type":"markdown","source":"![](https://www.xenonstack.com/wp-content/uploads/xenonstack-credit-card-fraud-detection.png)"},{"metadata":{},"cell_type":"markdown","source":"The kernels below helped me in writing this kernel. Thanks!\n\nAndrew Lukyanenko: https://www.kaggle.com/artgor/eda-and-models\n\nLeonardo Ferreira: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-simple-lgbm"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\nfrom sklearn import preprocessing\n\nimport gc, datetime, random\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions used for visualization are below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workarounds = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workarounds\n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's check the sample submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsample_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are supposed to output the probability of a transaction being fraudulant. "},{"metadata":{"trusted":true},"cell_type":"code","source":"del sample_sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will load the train and test data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data seems quite huge and hard to understand. TransactionID is the common column in both transaction data and identity data and the two tables can be joined using this common column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train dataset: {train.shape[0]} rows & {train.shape[1]} columns')\nprint(f'Test dataset: {test.shape[0]} rows & {test.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anyways, we need to analyze the data to find out which fields are useful and which are not."},{"metadata":{},"cell_type":"markdown","source":"## Reduce Mamory"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have merged the train_transaction and train_identity into a single table called train (similarly for test data). So we can delete the extra info."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity, train_transaction, test_identity, test_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recognize categorical and numerical attributes"},{"metadata":{},"cell_type":"markdown","source":"Now let's recognize the categorical data and numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = list(train.select_dtypes(include=['object']).columns)\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above,\n\n       ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1',\n       'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15',\n       'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33',\n       'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n       'DeviceInfo']\n       \nare categorical and the rest of features are numerical."},{"metadata":{},"cell_type":"markdown","source":"# Recognize missing data"},{"metadata":{},"cell_type":"markdown","source":"In a real dataset, it is common to have many null attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_null = train.isnull().sum()/len(train) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:500]\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 414 attributes containing null values."},{"metadata":{},"cell_type":"markdown","source":"Now we will delete attributes with more than 90 percent missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.9]\n    return many_null_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return big_top_value_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 90% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    cols_to_drop.remove('isFraud')\n    return cols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = get_useless_columns(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(40,10))\nplt.xticks(rotation='90')\nsns.barplot(data_null.index, data_null)\nplt.xlabel('Features', fontsize=20)\nplt.ylabel('Missing rate', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Knowing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_03'].value_counts(dropna=False, normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, 88% of values are missing and 10% of them are equal to 0. So, 98% of data is either missing or 0. This attribute does not seem to be helpful."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_11'].value_counts(dropna=False, normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that 76% of data is missing and more that 22% is equal to 100. This does not seem useful either."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    print(train['id_0' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')\n    \nfor i in range(10, 39):\n    print(train['id_' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the categorical ones which are:\n\n['id_12',\n 'id_15',\n 'id_16',\n 'id_23',\n 'id_27',\n 'id_28',\n 'id_29',\n 'id_30',\n 'id_31',\n 'id_33',\n 'id_34',\n 'id_35',\n 'id_36',\n 'id_37',\n 'id_38']"},{"metadata":{"trusted":true},"cell_type":"code","source":"charts = {}\ninfo = []\nfor i in range(12, 39):\n    info.append('id_' + str(i))\nfor i in info:\n    width_len = 400\n    if i in ['id_30', 'id_31', 'id_33']:\n        width_len = 600\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=width_len)\n    charts[i] = chart                         \n\n\nfor i in ['id_30', 'id_31', 'id_33']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender((charts['id_12'] | charts['id_15']) & \n       (charts['id_16'] | charts['id_23']) & \n       (charts['id_27'] | charts['id_28']) & \n       (charts['id_29'] | charts['id_34']) & \n       (charts['id_35'] | charts['id_36']) &\n       (charts['id_37'] | charts['id_38']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"render(charts['id_30'] & charts['id_31'] & charts['id_33'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DeviceType and DeviceInfo"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender(charts['DeviceType'] & charts['DeviceType'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Transaction dates');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, it is shown that the dates of Train and Test data have an empty intersection."},{"metadata":{},"cell_type":"markdown","source":"### Card"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 7):\n    print(train['card' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 15):\n    print(train['C' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### D"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 16):\n    print(train['D' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### M"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    print(train['M' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"charts = {}\ninfo = []\nfor i in range(1, 10):\n    info.append('M' + str(i))\nfor i in info:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['M1'] | charts['M2'] | charts['M3']) & (charts['M4'] | charts['M5'] | charts['M6']) & (charts['M7'] | charts['M8'] | charts['M9']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del charts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nseed_everything(SEED)\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = addNewFeatures(train)\ntest = addNewFeatures(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.replace(np.inf,999)\ntest = test.replace(np.inf,999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Email Domains"},{"metadata":{},"cell_type":"markdown","source":"As you may have noticed, for some companies there are several email addresses. For a better analysis we will consider them same as each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle P Email Domain and R Email Domain"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"def setTime(df):\n    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    # Lets transform D8 and D9 column\n    # As we almost sure it has connection with hours\n    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n    df['D8'] = df['D8'].fillna(-1).astype(int)\n\n    return df\n    \ntrain=setTime(train)\ntest=setTime(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Browser Version"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain=setBrowser(train)\ntest=setBrowser(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Device Type"},{"metadata":{},"cell_type":"markdown","source":"We have the same issue with devices too."},{"metadata":{"trusted":true},"cell_type":"code","source":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    \n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain=setDevice(train)\ntest=setDevice(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] /= train[period+'_total']\n        test[new_column]  /= test[period+'_total']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Data for Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedLabelEncoder(LabelEncoder):\n    def fit_transform(self, y, *args, **kwargs):\n        return super().fit_transform(y).reshape(-1, 1)\n\n    def transform(self, y, *args, **kwargs):\n        return super().transform(y).reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attributes = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attributes].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_cols = [\n    'TransactionID','TransactionDT',                      # Not target in features))\n    'uid','uid2','uid3',                                 \n    'DT','DT_M','DT_W','DT_D',       \n    'DT_hour','DT_day_week','DT_day',\n    'DT_D_total','DT_W_total','DT_M_total',\n    'id_30','id_31','id_33',\n    'D1', 'D2', 'D9',\n]\n\nnoisy_cat_cols = list(train[noisy_cols].select_dtypes(include=['object']).columns) \nnoisy_num_cold = list(train[noisy_cols].select_dtypes(exclude=['object']).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attr = list(train.select_dtypes(include=['object']).columns)\nnum_attr = list(train.select_dtypes(exclude=['object']).columns)\nnum_attr.remove('isFraud')\n\nfor col in noisy_cat_cols:\n    if col in cat_attr:\n        print(\"Deleting \" + col)\n        cat_attr.remove(col)\nfor col in noisy_num_cold:\n    if col in num_attr:\n        print(\"Deleting \" + col)\n        num_attr.remove(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([\n        ('selector', DataFrameSelector(num_attr)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('scaler', StandardScaler()),\n    ]) \n\ncat_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n    ])\n\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to deal with categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encodeCategorical(df_train, df_test):\n    for f in df_train.drop('isFraud', axis=1).columns:\n        if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n            df_train[f] = lbl.transform(list(df_train[f].values))\n            df_test[f] = lbl.transform(list(df_test[f].values))\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud']\ntrain, test = encodeCategorical(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(full_pipeline.fit_transform(train))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models"},{"metadata":{},"cell_type":"markdown","source":"We need to find the best model and train it."},{"metadata":{},"cell_type":"markdown","source":"We prepare the test data with the pipeline we had so it will be ready to be used for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(cols_to_drop, axis=1)\ntest = pd.DataFrame(full_pipeline.transform(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"def makePredictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df, y_train    \n    P = tt_df\n\n    predictions = np.zeros(len(tt_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    \n    return tt_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.064,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params['learning_rate'] = 0.005\nlgb_params['n_estimators'] = 1800\nlgb_params['early_stopping_rounds'] = 100    \ntest_predictions = makePredictions(X_train, test, TARGET, lgb_params, NFOLDS=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_submission = pd.DataFrame({\n    \"isFraud\": test_predictions['prediction'],\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_submission.insert(0, \"TransactionID\", np.arange(3663549, 3663549 + 506691))\nlgb_submission.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}