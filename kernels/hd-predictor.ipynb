{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to [IntiPic](https://www.kaggle.com/intipic) for explaining the dataset in [this](https://www.kaggle.com/ronitf/heart-disease-uci/discussion/105877) post."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nseed = 51","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature engineering: There is a hypothesis that high cholesterol actually protects elderly patients. So lets create chol_age feature that reduces the value of chol as we age:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['chol_age'] = data['chol']/data['age']\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fix the scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\ndata['age'] = RobustScaler().fit_transform(data['age'].values.reshape(-1, 1))\ndata['chol_age'] = RobustScaler().fit_transform(data['chol_age'].values.reshape(-1, 1))\ndata['trestbps'] = RobustScaler().fit_transform(data['trestbps'].values.reshape(-1, 1))\ndata['chol'] = RobustScaler().fit_transform(data['chol'].values.reshape(-1, 1))\ndata['thalach'] = RobustScaler().fit_transform(data['thalach'].values.reshape(-1, 1))\ndata['oldpeak'] = RobustScaler().fit_transform(data['oldpeak'].values.reshape(-1, 1))\n\ndata.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make some of the values more clear:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cp'][data['cp'] == 0] = 'asymptomatic'\ndata['cp'][data['cp'] == 1] = 'atypical angina'\ndata['cp'][data['cp'] == 2] = 'non-anginal pain'\ndata['cp'][data['cp'] == 3] = 'typical angina'\n\ndata['restecg'][data['restecg'] == 0] = 'left ventricular hypertrophy'\ndata['restecg'][data['restecg'] == 1] = 'normal'\ndata['restecg'][data['restecg'] == 2] = 'ST-T wave abnormality '\n\ndata['slope'][data['slope'] == 0] = 'down'\ndata['slope'][data['slope'] == 1] = 'flat'\ndata['slope'][data['slope'] == 2] = 'up'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check correlation of each attribute to the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr.sort_values([\"target\"], ascending = False, inplace = True)\ncorr.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible that our engineered feature, chol_age, is proving the hypothesis by showing a slight correlation with the abscense of heart disease. In the future it might be interesting to replace the chol feature with our new chol_age feature.\n\nLet's take a closer look at cp, slope, restcg, and thal correlation. Here is what they mean:\n\ncp: chest pain type\n* -- Value 0: asymptomatic\n* -- Value 1: atypical angina\n* -- Value 2: non-anginal pain\n* -- Value 3: typical angina\n\nslope: the slope of the peak exercise ST segment\n* 0: downsloping; \n* 1: flat; \n* 2: upsloping\n\nrestecg: resting electrocardiographic results\n* -- Value 0: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* -- Value 1: normal\n* -- Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n\nthal: \n* 1 = fixed defect; \n* 2 = normal; \n* 7 = reversable defect\n\nSo, let's one hot encode them"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nOH_cols = ['cp', 'slope', 'restecg','thal']\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_data = pd.DataFrame(OH_encoder.fit_transform(data[OH_cols]))\n\n# One-hot encoding put in generic column names, use feature names instead\nOH_cols_data.columns = OH_encoder.get_feature_names(OH_cols)\n\n# # remove the original columns\n# for c in OH_cols:\n#     cols_to_use.remove(c)\n    \n# # Add one-hot columns to cols_to_use\n# for c in OH_cols_data.columns:\n#     cols_to_use.append(c)\n\n# # print(cols_to_use)\n\n# One-hot encoding removed index; put it back\nOH_cols_data.index = data.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_data = data.drop(OH_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_data = pd.concat([num_data, OH_cols_data], axis=1)\n\ndata = OH_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets check the correlation again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr.sort_values([\"target\"], ascending = False, inplace = True)\ncorr.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected chol has very little correlation with the target. **Note** that a more negative number correlates with having the disease (target = 0). A more positive number correlates with not having the disease (target = 1). A small number means there is little correlation with the target.\n\nHighest correlations are with asymptomatic chest pain, thal_3(probably reversible defect), and exang(angina=yes)."},{"metadata":{},"cell_type":"markdown","source":"Create some training and test data to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.drop(['target'], axis=1)\ny = data['target']\n\ndef setup_data(X_in, y_in):\n    return train_test_split(X_in, y_in, test_size=0.2, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\ntensorflow.random.set_seed(seed) \nfrom tensorflow.keras.layers import Input, Dense, ELU, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\ninput = Input(shape=X.shape[1])\n\nm = Dense(1024)(input)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\n#####\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\n# m = Dense(16, kernel_regularizer=l2(0.01))(m)\n# m = ELU()(m)\n\noutput = Dense(1, activation='sigmoid')(m)\n\nmodel = Model(inputs=[input], outputs=[output])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n\nes = EarlyStopping(monitor='val_loss', patience=200, verbose=1, restore_best_weights=True)\n\nrlp = ReduceLROnPlateau(monitor='val_loss', patience=9, verbose=1, factor=0.5, cooldown=5, min_lr=1e-10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's set aside 20% of the data as a test set and use the rest for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_remainder, X_test, y_remainder, y_test = setup_data(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation, y_train, y_validation = setup_data(X_remainder, y_remainder)\n\nhistory = model.fit(X_train,\n    y_train,\n    batch_size=64,\n    epochs=200,\n    verbose=2,\n    callbacks=[es, rlp],\n    validation_data=(X_validation, y_validation),\n    shuffle=True\n         ).history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n\nax1.plot(history['loss'], label='Train loss')\nax1.plot(history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history['acc'], label='Train accuracy')\nax2.plot(history['val_acc'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's continue to shuffle the data and train more"},{"metadata":{"trusted":true},"cell_type":"code","source":"rlp = ReduceLROnPlateau(monitor='val_loss', patience=9, verbose=0, factor=0.5, cooldown=5, min_lr=1e-10)\n\nfor z in range(5):\n\n    X_train, X_validation, y_train, y_validation = setup_data(X_remainder, y_remainder)\n\n    history = model.fit(X_train,\n        y_train,\n        batch_size=64,\n        epochs=200,\n        verbose=0,\n        callbacks=[es, rlp],\n        validation_data=(X_validation, y_validation),\n        shuffle=True\n             ).history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For curiosity's sake, let's evaluate the model on the full data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X, y, batch_size=64, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears our model can predict the target accurately. But of course this would be a lot more interesting with a much larger training dataset and a separate test dataset.\n\nOK. Let's do a final training on the full training set without validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_remainder,\n    y_remainder,\n    batch_size=64,\n    epochs=200,\n    verbose=2,\n    callbacks=[es, rlp],\n    shuffle=True\n         ).history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how it does for the test data we set aside earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So we are over 80% accurate. Pretty reasonable for such a small dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_prob = model.predict(X_test)\ny_pred = np.around(y_prob)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconfusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sensitivity = true positive rate, Specificity = true negative rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}