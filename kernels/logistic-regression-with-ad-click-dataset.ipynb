{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# Logistic Regression Project - Solutions\n\nthis project contains a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement on a company website. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n\nThis data set contains the following features:\n\n* 'Daily Time Spent on Site': consumer time on site in minutes\n* 'Age': cutomer age in years\n* 'Area Income': Avg. Income of geographical area of consumer\n* 'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n* 'Ad Topic Line': Headline of the advertisement\n* 'City': City of consumer\n* 'Male': Whether or not consumer was male\n* 'Country': Country of consumer\n* 'Timestamp': Time at which consumer clicked on Ad or closed window\n* 'Clicked on Ad': 0 or 1 indicated clicking on Ad\n\n## Import Libraries\n\n**Import a few libraries you think you'll need (Or just import them as you go along!)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## Get the Data\n**Read in the advertising.csv file and set it to a data frame called ad_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data = pd.read_csv('../input/advertising/advertising.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check the head of ad_data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Use info and describe() on ad_data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.columns # displays column names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nLet's use seaborn to explore the data!\n\nTry recreating the plots shown below!\n\n** Create a histogram of the Age**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nad_data['Age'].hist(bins=30)\nplt.xlabel('Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(ad_data['Country'], ad_data['Clicked on Ad']).sort_values( 1,ascending = False).tail(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data[ad_data['Clicked on Ad']==1]['Country'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ad_data['Country'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=ad_data['Country'],columns='count').sort_values(['count'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that users are from all over the world with maximum from france and czech republic with a count of 9 each.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Check for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let us begin to focus on time information. What is the data type of the objects in the timeStamp column?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(ad_data['Timestamp'][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** You should have seen that these timestamps are still strings. Use [pd.to_datetime](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) to convert the column from strings to DateTime objects. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract datetime variables using timestamp column\nad_data['Timestamp'] = pd.to_datetime(ad_data['Timestamp']) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting timestamp column into datatime object in order to extract new features\nad_data['Month'] = ad_data['Timestamp'].dt.month \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a new column called Month\nad_data['Day'] = ad_data['Timestamp'].dt.day     \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a new column called Day\nad_data['Hour'] = ad_data['Timestamp'].dt.hour   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a new column called Hour\nad_data[\"Weekday\"] = ad_data['Timestamp'].dt.dayofweek \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping timestamp column to avoid redundancy\nad_data = ad_data.drop(['Timestamp'], axis=1) # deleting timestamp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can also  use .apply() to create 3 new columns called Hour, Month, and Day of Week. You will create these columns based off of the timeStamp column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ad_data['Hour'] = ad_data['timeStamp'].apply(lambda time: time.hour)\n# ad_data['Month'] = ad_data['timeStamp'].apply(lambda time: time.month)\n# ad_data['Day of Week'] = ad_data['timeStamp'].apply(lambda time: time.dayofweek)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Clicked on Ad', data = ad_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Jointplot of daily time spent on site and age \nsns.jointplot(x = \"Age\", y= \"Daily Time Spent on Site\", data = ad_data) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that more people aged between 30 to 40 are spending more time on site daily."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# scatterplot of daily time spent on site and age with clicking ads as hue\nsns.scatterplot(x = \"Age\", y= \"Daily Time Spent on Site\",hue='Clicked on Ad', data = ad_data) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that more people aged between 20 to 40 are spending more time on site daily but less chances of them to click on the ads."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Jointplot of daily time spent on site and age clicking ads as hue\nsns.lmplot(x = \"Age\", y= \"Daily Time Spent on Site\",hue='Clicked on Ad', data = ad_data) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that people that are younger and spends more time on site  click on the ads less and people who are in between 25-55 and spends less time click on the ads more."},{"metadata":{},"cell_type":"markdown","source":"## Distribution and Relationship Between Variables"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Creating a pairplot with hue defined by Clicked on Ad column\nsns.pairplot(ad_data, hue = 'Clicked on Ad', vars = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage'],palette = 'rocket')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pairplot represents the relationship between our target feature/variable and explanatory variables. It provides the possible direction of the relationship between the variables. We can see that people who spend less time on site and have less income and are aged more relatively are tend to click on ad."},{"metadata":{"trusted":true},"cell_type":"code","source":"plots = ['Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage']\nfor i in plots:\n    plt.figure(figsize = (12, 6))\n    \n    plt.subplot(2,3,1)\n    sns.boxplot(data= ad_data, y=ad_data[i],x='Clicked on Ad')\n    plt.subplot(2,3,2)\n    sns.boxplot(data= ad_data, y=ad_data[i])\n    plt.subplot(2,3,3)\n    sns.distplot(ad_data[i],bins= 20,)       \n    plt.tight_layout()\n    plt.title(i)    \n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('oldest person didn\\'t clicked on the ad was of was of:', ad_data['Age'].max(), 'Years')\nprint('oldest person who clicked on the ad was of:', ad_data[ad_data['Clicked on Ad']==0]['Age'].max(), 'Years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Youngest person was of:', ad_data['Age'].min(), 'Years')\nprint('Youngest person who clicked on the ad was of:', ad_data[ad_data['Clicked on Ad']==0]['Age'].min(), 'Years')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average age was of:', ad_data['Age'].mean(), 'Years')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12,10))\nsns.heatmap(ad_data.corr(), cmap='viridis', annot = True) \n# Degree of relationship i.e correlation using heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Heatmap gives us better understanding of relationship between each feature. Correlation is measured between -1 and 1. Higher the absolute value, higher is the degree of correlation between the variables. We expect daily internet usage and daily time spent on site to be more correlated with our target variable. Also, none of our explantory variables seems to correlate with each other which indicates there is no collinearity in our data."},{"metadata":{},"cell_type":"markdown","source":"# Data visualization with Time, Days and Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(14,5))\nad_data['Month'][ad_data['Clicked on Ad']==1].value_counts().sort_index().plot(ax=ax[0])\nax[0].set_ylabel('Count of Clicks')\npd.crosstab(ad_data[\"Clicked on Ad\"], ad_data[\"Month\"]).T.plot(kind = 'Bar',ax=ax[1])\n#ad_data.groupby(['Month'])['Clicked on Ad'].sum() \nplt.tight_layout()\nplt.suptitle('Months Vs Clicks',y=0,size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(14,5))\npd.crosstab(ad_data[\"Clicked on Ad\"], ad_data[\"Hour\"]).T.plot(style = [], ax = ax[0])\npd.pivot_table(ad_data, index = ['Weekday'], values = ['Clicked on Ad'],aggfunc= np.sum).plot(kind = 'Bar', ax=ax[1]) # 0 - Monday\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nNow it's time to do a train test split, and train our model!\n\nYou'll have the freedom here to choose columns that you want to train on!"},{"metadata":{},"cell_type":"markdown","source":"** Split the data into training set and testing set using train_test_split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = ad_data[['Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage', 'Male']]\ny = ad_data['Clicked on Ad']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Train and fit a logistic regression model on the training set.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel = LogisticRegression(solver='lbfgs')\nlogmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and Evaluations\n** Now predict values for the testing data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = logmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Create a classification report for the model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Importing a pure confusion matrix from sklearn.metrics family\nfrom sklearn.metrics import confusion_matrix\n\n# Printing the confusion_matrix\nprint(confusion_matrix(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results from evaluation are as follows:\n\nConfusion Matrix:\n\nThe users that are predicted to click on commercials and the actually clicked users were 154, the people who were predicted not to click on the commercials and actually did not click on them were 170.\n\nThe people who were predicted to click on commercial and actually did not click on them are 1, and the users who were not predicted to click on the commercials and actually clicked on them are 5.\n\nWe have only a few mislabelled points which is not bad from the given size of the dataset.\n\nClassification Report:\n\nFrom the report obtained, the precision & recall are 0.98 which depicts the predicted values are 98% accurate. Hence the probability that the user can click on the commercial is 0.98 which is a great precision value to get a good model.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}