{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Iris Data Set Visualization and Machine Learning Implementation"},{"metadata":{},"cell_type":"markdown","source":"## Goal: To classify the species of flower based on their attributes"},{"metadata":{},"cell_type":"markdown","source":"## Importing our dataset and creating a DataFrame for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Importing basic Python libraries necessary for data manipulation and visualization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n##Enabling matplotlib in jupyter notebook\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Loading and examining the Iris dataset file into a Pandas DataFrame for analysis.\n#Creating a DataFrame with variable name 'iris'.\n\niris = pd.read_csv ('../input/iris/Iris.csv')\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning Id column as the index for slightly easier data manipulation later on.\n\niris.set_index('Id', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changing all column names to lowercase for easier selection (personal preference).\n\niris.columns = map(str.lower, iris)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2: Performing statistical analysis on the dataset, as well as checking for possible errors (missing values).\n\niris.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris[['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']].isna().describe()\n\n#Fortunately, there are no missing values in this dataset.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Step 3: Plotting a few graphs to gain a sensing of the relation between the features.\n#Using Seaborn's pairplot to gain a broad overview of the dataset.\n\nsns.pairplot(iris, hue = 'species', diag_kind = 'hist', palette = 'Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Seaborn's jointplot for a slightly more indepth look.\n\nsns.jointplot(x = 'sepallengthcm', y = 'sepalwidthcm', data = iris)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Seaborn's scatterplot to obtain a similar plot, but color-coded by their species\n\nsns.scatterplot(x = 'sepallengthcm', y = 'sepalwidthcm', data = iris, hue = 'species', palette = 'Set1')\nplt.title('Sepal Width vs Sepal Length')\nplt.legend(loc = 'center left', bbox_to_anchor = (1.0, 0.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plots, namely the Pairplot and Scatterplot, we can see that there is already one species which stands out from the rest, relatively speaking; the Setosa species. From the graphs, it seems that this particular species has features which stand out the most distinctively from the other two species."},{"metadata":{"trusted":true},"cell_type":"code","source":"##We shall now continue with the plots, with a deeper look into the distribution and correlation between the features.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Using matplotlib to create a figure for a more compressed view of the four features.\n#Using histograms for an overview of the varying values of each feature.\n\niris.hist(figsize = (12, 12), ec = 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using matplotlib to create a figure consisting of four subplots.\n#Using Seaborn's boxplot to gain insight into the distribution of the four features.\n\nplt.figure(figsize = (12, 12))\nsns.set_style('whitegrid')\nsns.set_palette('Set1')\nplt.subplot(2, 2, 1)\nsns.boxplot(x = 'species', y = 'petallengthcm', data = iris)\nplt.subplot(2, 2, 2)\nsns.boxplot(x = 'species', y = 'petalwidthcm', data = iris)\nplt.subplot(2, 2, 3)\nsns.boxplot(x = 'species', y = 'sepallengthcm', data = iris)\nplt.subplot(2, 2, 4)\nsns.boxplot(x = 'species', y= 'sepalwidthcm', data = iris,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, I wish to show the correlation between the features(if any), using Seaborn's heatmap feature.\n#Firstly, doing some basic transformation of the pandas DataFrame.\n\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_corr = iris.corr()\niris_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Seaborn's heatmap to visualize the correlation.\n\nsns.heatmap(iris_corr, cbar = True, annot = True, cmap = 'RdBu_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having done some exploratory data analysis, it is time to move on to training and implementing our machine learning algorithms."},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Algorithms"},{"metadata":{},"cell_type":"markdown","source":"The end goal of this analyis is to successfully create a model which is able to classify the species of the flower based on their attributes. Given that this is a classification problem, it makes sense that we try to employ the relevant algorithms to achieve this.\n\nIn particular, we will strive to implement the following Machine Learning models:\n1. Logistic Regression\n2. K-Nearest Neighbours\n3. Decision Trees/Random Forests\n5. Support Vector Machines\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Logistic regression is a reasonably straightforward linear statistical model which allows us to predict the binomial\noutcome of one or more variables. \n\nIn our case, we will be measuring the relationships between our sole categorical variable (the species of flower), and the rest of our independent variables (the sepal length/width and petal length/width).\n\nUltimately, the model will estimate the probability of a certain data point of being a particular species of flower using a logistic function (the sigmoid function)."},{"metadata":{"trusted":true},"cell_type":"code","source":"##I will be going through my process of implementing logistic regression in the following rows using a step-by-step\n##approach once again.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Creating a training and test set (look up Google for reasons on why we need to split a training and test set)\n\n#Brief review of our DataFrame:\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our matrix of flower features and their respective values for each data point, X.\n\n#We drop the 'species' column since it is our dependent variable in this case.\nX = iris.drop('species', axis = 1) \nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning our dependent variable, the species of the flower, to y.\n\ny = iris['species']\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From here, we will be importing relevant modules from libraries to allow us to create our training and test data sets.\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with this, our training(X_train, y_train) and test(X_test, y_test) sets have been created.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 2: Training our model by fitting it to the training data sets.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing our LogisticRegression module and creating an instance to carry out the training of the model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogm = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model to our training data.\n\nlogm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 3: Obtaining our predictions by using the model with the test set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg_predictions = logm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 4: Evaluating our results by comparing the predicted results (lg_predictions) to the actual results (y_test).\n#To do this, I will import a few modules which allows us to visualize this comparison.\n\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, lg_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, lg_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, we can see that our Logistic Regression Model did reasonably well to predict the species of flower with a good accuracy and f1-scores. However, it may be too soon to conclude whether this is the best model thus far given the relatively simple dataset. Perhaps we can achieve better scores with the other training models which we will be carrying out."},{"metadata":{},"cell_type":"markdown","source":"### 2. K-Nearest Neighbours (kNN)"},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbours is another relatively simple machine learning algorithm which be used for both classification and regression problems. It works on the assumption that data points which share similar features will be clustered together. Hence, this allows us to determine a boundary between these 'clusters' and correspondingly group them using our predictions from the training model.\n\nThe main challenge when it comes to implementing this algorithm would be choosing the value of 'k', which corresponds to the number of data points closest to the data point we are examining we should take into account to come up with our predictions. If 'k' is too small, we may end up jumping to conclusions. If 'k' is too large', it becomes difficult to accurately create our 'clusters'.\n\nFortunately, from the above Exploratory Data Analysis we have carried out, we can see that the data points have already been nicely 'clustered' for us. Hence, implementing kNN should proceed rather smoothly."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Creating a training and test data set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = iris.drop('species', axis = 1)\ny = iris['species']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 2: Implementing kNN, starting with an arbitrary value of k=5.\n#Now, we will again import the kNeighborsClassifier to carry out our implementation and learning.\n\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating an instance and fitting it to our training data.\n\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean') \nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our predictions.\n\nknn_predictions = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 3: Evaluating our predicted results against the actual results.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, knn_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, knn_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 4: Choosing the 'best' value of 'k'\n#In order to do this, we can plot a graph of the error-rate of the model against the k-value that is being selected.\n#This allows us to get a better gauge of the domain of k values which may allow us to obtain a better accuracy on our\n#model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use a for loop to repeat steps 3-4 using different values of k.\nerror_rate = []\n\nfor i in range (1, 30):\n    knn_i = KNeighborsClassifier(n_neighbors = i)\n    knn_i.fit(X_train, y_train)\n    knn_i_pred = knn_i.predict(X_test)\n    error_rate.append(np.mean (knn_i_pred != y_test))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nplt.plot(np.arange(1, 30), error_rate, 'o-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seems like our initial value of k=5 was a pretty good estimate. Hence, we shall stick with it.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Decision Trees/Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Decision trees are another classification/regression algorithm of which the objective is to predict the value of a variable using certain 'rules' via a tree based off the features of the data points.\n\nIn layman terms, it could be said to follow a 'if this feature is true, then proceed here, else the other side' principle. For more information, do look up Google or other sources for a better explanation."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Creating a training/test data set.\n#X and y have already been defined from above examples.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 2: Importing our module to fit and train the model for predictions.\n\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_predictions = tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 3: Evaluating our metrics given the trained model.\n\nprint(confusion_matrix(y_test, tree_predictions))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, tree_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##We shall now carry on with Random Forests implementation. For more\n##details regarding this model, please look up details from other\n##sources for more detailed explanations.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Importing our random forests module and training it.\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X_train, y_train)\nrfc_predictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 2: Evaluating our metrics given the predicted values.\n\nprint(confusion_matrix(y_test, rfc_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, rfc_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we got similar results to our DecisionTreeClassifier.\nOriginally, the purpose of Random Forests was to select a random sample of features for every tree at every split in the decision tree. This could serve the purpose of reducing high variance by 'decorrelating' the original decision trees, especially if there is a particular feature which has a proportionately larger impact on the outcome than the other features.\n\nIn such cases, depending on whether our end goal is to obtain high precision or high recall, a decision on which model to use will have to be made."},{"metadata":{},"cell_type":"markdown","source":"### 4. Support Vector Machines"},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machines work by learning from the data to fit a dividing hyperplane which best separates the data points based on their cluster. This can also be seen as something like a 'decision boundary' for those who may be more familiar with this term. Nevertheless, it can be used for both regression and classification problems, and is suitable for our dataset here."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 1: Creating a training and test data set.\n##Since the steps have been repeated multiple times in the above few\n##examples, I shall be skipping this step from here on.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 2: Importing our SVM classifier and training it.\n\nfrom sklearn.svm import SVC\nsvc = SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc.fit(X_train, y_train)\nsvc_predictions = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Step 3: Evaluating our metrics given predicted values.\n\nprint(confusion_matrix(y_test, svc_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, svc_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With that, we have completed the implementation of the ML algorithms which we set out to do. There are definitely other methods which could be used, but I shall leave that to the curious mind to explore more. As this is my first project, please do go easy on me and feel free to point out any mistakes or questions. Thank you!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}