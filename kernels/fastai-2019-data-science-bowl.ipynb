{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\n\nfrom tqdm.notebook import tqdm\nimport json\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular import * \n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_columns', None)\npd.set_option('display.min_rows', 100)\npd.set_option('display.max_rows', 100)\nhome = Path(\"/kaggle/input/data-science-bowl-2019/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My approach to 2019 DS bowl with fastai v1\n\nI used these awesome notebooks:\n    \n    https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n    https://www.kaggle.com/amanooo/dsb-2019-regressors-and-optimal-rounding\n    https://www.kaggle.com/tarandro/regression-with-less-overfitting"},{"metadata":{},"cell_type":"markdown","source":"* [Looking at data](#Looking-at-data)\n* [Preparing data](#Preparing-data)\n* [Approach](#How-to-approach)\n* [Train](#Train)\n* [Submission](#Submission)"},{"metadata":{},"cell_type":"markdown","source":"# Looking at data"},{"metadata":{"trusted":true},"cell_type":"code","source":"specs = pd.read_csv(home/\"specs.csv\"); len(specs)\nspecs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(home/\"train_labels.csv\"); len(train_labels)\ntrain_labels.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv(home/\"sample_submission.csv\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntypes = {\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32}\nraw_train = pd.read_csv(home/\"train.csv\", dtype=types)\nraw_train[\"timestamp\"] = pd.to_datetime(raw_train[\"timestamp\"]); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test = pd.read_csv(home/\"test.csv\", dtype=types)\nraw_test[\"timestamp\"] = pd.to_datetime(raw_test[\"timestamp\"])\nraw_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems `game_time` is not captured correctly - there's a huge window in between some events in a given session."},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw_train[raw_train[\"game_session\"] == \"969a6c0d56aa4683\"].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target.\n\nFirst we will look at the target we intend to predict.\n\nWe are told: The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). The outcomes in this competition are grouped into 4 groups (labeled `accuracy_group` in the data):\n\n    3: the assessment was solved on the first attempt\n    2: the assessment was solved on the second attempt\n    1: the assessment was solved after 3 or more attempts\n    0: the assessment was never solved\n\nFor each installation_id represented in the test set, you must predict the accuracy_group **of the last assessment** for that installation_id"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We are told in the data description that:\n\n* The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set.\n* Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110.\n* If the attempt was correct, it contains \"correct\":true.\n\nWe also know:\n\n* The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt).\n* Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices.\n* In the training set, you are provided **the full history of gameplay data.**\n* In the test set, **we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.**\n* Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n"},{"metadata":{},"cell_type":"markdown","source":"# How to approach\n\nFor test set we guess the group based on **single** installation_id. But the train\\test datasets contain **repeatable** installation ids with different game sessions.\nHence it makes sense to guess the group for each assessment using history.\n\nThe questions:\n* does randomly truncated history in test conflicts with the above?\nFrom the test dataset, the last asssessment data is cleaned. So it looks like a session with only 1 event.\nTo have a good validation dataset however, it should be the same as test - https://www.kaggle.com/tarandro/regression-with-less-overfitting\n* Is more than 1 correct submission impossible per session? Does it mean noise - two devices with the same id sharing the same session?"},{"metadata":{},"cell_type":"markdown","source":"Remove `installation_id` without any assesments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO keep them and see how it affects score\nids_with_subms = raw_train[raw_train.type == \"Assessment\"][['installation_id']].drop_duplicates()\nraw_train = pd.merge(raw_train, ids_with_subms, on=\"installation_id\", how=\"inner\"); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(correct_data):\n    # Rounding correct > 1 to 1 lowers the score. Why?\n    correct = len(correct_data.loc[correct_data])\n    wrong = len(correct_data.loc[~correct_data])\n    accuracy = correct/(correct + wrong) if correct + wrong else 0\n    return accuracy, correct, wrong\n\ndef get_group(accuracy):\n    if not accuracy:\n        return 0\n    elif accuracy == 1:\n        return 3\n    elif accuracy >= 0.5:\n        return 2\n    return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I prefer this over calculating average\ndef lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRY\n# do not remove assessments without attempts in train\n# add number of passed assessments\n# add time spent in each activity\n# remove sessions with 1 row\n# clear installation_id not found in train_labels\n# event_data\n\ndef prepare(data: pd.DataFrame, one_hot: List[str], test=False) -> pd.DataFrame:\n    one_hot_dict = defaultdict(int)\n\n    prepared = []\n    for id_, g in tqdm(data.groupby(\"installation_id\", sort=False)):\n        features = process_id(g, one_hot, one_hot_dict.copy(), test)\n        if not features:\n            continue\n        if test:\n            features[-1][\"is_test\"] = 1\n        prepared.extend(features)\n    return pd.DataFrame(prepared).fillna(0).sort_index(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_id(id_data: pd.DataFrame, one_hot_cols, one_hot_dict, test: bool) -> pd.DataFrame:\n    a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean = 0, 0, 0, 0, 0, 0\n    a_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n    # accumulated one_hot features per id for a given session, e.g. Bird Measurer: 50\n    features = []\n\n    for s, gs in id_data.groupby(\"game_session\", sort=False):\n        def update_counter(counter: dict, column: str):\n            session_counter = Counter(gs[column])\n            for value in session_counter.keys():\n                counter[f\"{column}_{value}\"] += session_counter[value]\n            return counter\n\n        def process_session(gs):\n            # share state with parent process_id()\n            nonlocal one_hot_dict, a_groups, a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean\n            # increment one hot columns for session, e.g. Bird Measurer: 50\n            for c in one_hot_cols:\n                one_hot_dict.update(update_counter(one_hot_dict, c))\n    \n            # an accumulated session duration mean\n            duration = (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds\n            accumulated_duration_mean = lin_comb(accumulated_duration_mean or duration, duration, beta=0.9)\n            if gs[\"type\"].iloc[0] != \"Assessment\":\n                return\n\n            guess_mask = ((gs[\"event_data\"].str.contains(\"correct\")) & \n             (((gs[\"event_code\"] == 4100) &(~gs[\"title\"].str.startswith(\"Bird\")) | \n               ((gs[\"event_code\"] == 4110) & (gs[\"title\"].str.startswith(\"Bird\"))))))\n            answers = gs.loc[guess_mask].event_data.apply(lambda x: json.loads(x).get(\"correct\"))\n\n            # skip assessments without attempts in train\n            if answers.empty and not test:\n                return\n            accuracy, correct, wrong = get_accuracy(answers)\n            assert accuracy <= 1\n            group = get_group(accuracy)\n            processed = {\"installation_id\": id_data.installation_id.iloc[0], #\"game_session\": s,\n                         \"title\": gs.title.iloc[0],\n                         \"last_timestamp\": gs.timestamp.iloc[-1], \"accumulated_duration_mean\": accumulated_duration_mean,\n                         \"accumulated_correct\": a_correct, \"accumulated_incorrect\": a_wrong,\n                         \"accumulated_accuracy_mean\": a_accuracy/counter if counter > 0 else 0,\n                         \"accumulated_accuracy_group_mean\": a_group/counter if counter > 0 else 0, \n                         \"accuracy_group\": group}\n            processed.update(a_groups)\n            counter += 1\n            a_accuracy += accuracy\n            a_correct += correct\n            a_wrong += wrong\n            a_group += group\n            a_groups[str(group)] += 1\n            processed.update(one_hot_dict)\n            return processed\n        \n        gs.reset_index(inplace=True, drop=True)\n\n        if (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds > 1800:\n            gs.loc[:, \"passed\"] = gs.loc[:, \"timestamp\"].diff().apply(lambda x: x.seconds)\n            id_max = gs.loc[:, \"passed\"].idxmax()\n            if gs.loc[:, \"passed\"].max() > 1800:\n                session = gs.iloc[:id_max]\n                continued_session = gs.iloc[id_max:]\n                fs = process_session(session)\n                c_fs = process_session(continued_session)\n                if fs:\n                    features.append(fs)\n                if c_fs:\n                    features.append(c_fs)\n                continue\n\n        session_features = process_session(gs)\n        if session_features:\n            features.append(session_features)\n        \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gc; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_counters=[\"title\", \"type\", \"event_code\", \"event_id\"]\ntrain = prepare(raw_train, one_hot=one_hot_counters)\n# train = prepare(raw_train.iloc[:1_000_000], one_hot=one_hot_counters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(train, \"last_timestamp\", prefix=\"last_\", time=True)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = prepare(raw_test, one_hot=one_hot_counters, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the case when one hot encoded columns don't match between datasets\nadd_datepart(test, \"last_timestamp\", prefix=\"last_\", time=True);\n# diff = train.columns.difference(test.columns)\n# display(f\"Test doesn't contain {diff}\")\n# for c in diff:\n#     test[c] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# why discard good data from test, let's use all the taken assessments for train!\ntrain = (pd.concat([train, test[test[\"is_test\"] == 0].drop(columns=[\"is_test\"])],\n                   ignore_index=True, sort=False)).fillna(0)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.loc[test[\"is_test\"] == 1]\ntest.drop(columns=[\"accuracy_group\", \"is_test\"], inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del_cols = []\nfor col in train.columns.values:\n    counts = train[col].value_counts().iloc[0]\n    if (counts / train.shape[0]) >= 0.99:\n        del_cols.append(col)\ntrain.drop(columns=del_cols, inplace=True)\ntest.drop(columns=del_cols, inplace=True, errors=\"ignore\")\ndisplay(f\"Dropped {del_cols}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Proper validation dataset\n\nLet's assume the second hidden test is the same as this one. I.e. we predict the last assessment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab the last assessments per id\nvalid_idx = [g.iloc[-1].name for i, g in train.groupby(\"installation_id\", sort=False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threes = train.loc[valid_idx].query(\"accuracy_group == 3\").index\nzeroes = train.loc[valid_idx].query(\"accuracy_group == 0\").index\nothers = train.loc[valid_idx].query(\"accuracy_group == 1 or accuracy_group == 2\").index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_idx = sorted(pd.Series(threes).sample(len(others) // 2, random_state=42).to_list() +\n             pd.Series(zeroes).sample(len(others) // 2, random_state=42).to_list() +\n             others.to_list()\n            )\nlen(valid_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's randomly leave some data for the train\n# valid_idx = np.random.choice(valid_idx, int(len(valid_idx) * 0.5), replace=False)\n# valid_idx = pd.Series(valid_idx).sample(int(len(valid_idx) * 0.5), random_state=42).sort_values().values\n# len(valid_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ids = train[\"installation_id\"].unique()\n# sampled_ids = np.random.choice(ids, int(len(ids) * 0.2))\n# valid_idx = train[train[\"installation_id\"].isin(sampled_ids)].drop_duplicates([\"installation_id\"],\n#                                                                               keep=\"last\").index\n# len(valid_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[valid_idx].accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = train.columns[train.columns.str.startswith(\"last\", na=False)].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = \"accuracy_group\"\ncat_names = list(filter(lambda x: x not in [dep_var, \"last_Elapsed\"], date_cols)) + [\"title\"]\ncont_names = list(filter(lambda x: x not in [\"installation_id\", \"game_session\", dep_var] + cat_names,\n                         train.columns.to_list()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (TabularList.from_df(train, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs)\n        .split_by_idx(valid_idx=valid_idx)\n        .label_from_df(cols=dep_var, label_cls=CategoryList)\n        .add_test(TabularList.from_df(test, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs))\n        .databunch()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, ε:float=0.1, reduction='mean'):\n        super().__init__()\n        self.ε,self.reduction = ε,reduction\n    \n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return lin_comb(loss/c, nll, self.ε)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO - update kappa to regression\nlearn = tabular_learner(data, layers=[1000,500],\n#                         metrics=[mean_absolute_error, explained_variance],\n#                         y_range=[0, 3],\n                        metrics=[KappaScore(\"quadratic\")],\n                        loss_func=LabelSmoothingCrossEntropy(),\n#                         emb_drop=0.04,\n#                         use_bn=False,\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.model_dir = \"/kaggle/working\"\n# learn.lr_find()\n# learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-03)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for lr in [1e-03, 5e-03, 1e-02, 3e-02]:\n#     learn = tabular_learner(data, layers=[200,100],\n#                             metrics=[KappaScore(\"quadratic\")],\n#                             loss_func=LabelSmoothingCrossEntropy(),\n#                            )\n#     display(lr)\n#     learn.fit_one_cycle(20, lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds, y = learn.get_preds(ds_type=DatasetType.Test)\n# labels = preds.flatten()\n# display(labels[:10])\n# pd.Series(labels.tolist()).hist(bins=100); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def rounder(preds):\n#     y = preds.clone()\n#     y[y < 0.7] = 0\n#     y[(y >= 0.7) & (y < 1.4)] = 1\n#     y[y >= 1.8] = 3\n#     y[(y >= 1.4) & (y < 1.8)] = 2\n#     return y.type(torch.IntTensor)\n# labels = rounder(labels)\n# # labels = labels.round().type(torch.IntTensor)\n# pd.Series(labels.tolist()).hist(bins=4);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.Series(preds.type(torch.IntTensor).flatten()).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, y = learn.get_preds(ds_type=DatasetType.Test)\nlabels = np.argmax(preds, 1)\nsubmission = pd.DataFrame({\"installation_id\": test.installation_id, \"accuracy_group\": labels})\nsubmission.to_csv(\"submission.csv\", index=False)\nlen(submission), submission.accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}