{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello! I'm sharing a classification decision tree based on the CART algorithm, built from scratch and tested on the car evaluation data set, that I created as a personal project. I've also coded the evaluation metrics (accuracy, precision, recall, F1) and feature importances + visualize them with matplotlib."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decsion Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecisionTree:\n    \"\"\"A decision tree for classification based on the CART algorithm.\n\n    Args:\n        max_depth (int or None, default=None): The maximum depth of the tree. Used to control over-fitting.\n        min_samples_split (int, default=2): The minimum samples for splitting a node. Used to control over-fitting.\n\n    Attributes:\n        tree (Node object): The root node of the decision tree.\n        classes (ndarray): The class labels.\n    \"\"\"\n    def __init__(self, max_depth=None, min_samples_split=2):\n        self._max_depth = max_depth\n        self._min_samples_split = min_samples_split\n        self.tree = None\n        self.classes = None\n        self._feature_importances = None\n        self._n_samples = None\n        self._node_id = -1\n\n    def build(self, x, y):\n        \"\"\"Builds a decision tree based on the training set (x, y).\n\n        Args:\n            x (array-like, shape = [n_samples, n_features]): The training samples. Values must be numeric.\n            y (array-like, shape = [n_samples]): The class labels of samples in x. Values can be integers or strings.\n        \"\"\"\n        x, y = np.copy(x), np.copy(y)\n        self.classes = np.unique(y)\n        self._feature_importances = np.zeros((x.shape[1]))\n        self._n_samples = x.shape[0]\n        self.tree = self._build(x, y)\n\n    def _build(self, x, y, depth=0):\n        \"\"\"Recursively builds a decision tree.\n\n        Args:\n            x (array-like, shape = [n_samples, n_features]): The training samples.\n            y (array-like, shape = [n_samples]): The class labels of samples in x.\n            depth (int): The depth of the tree.\n\n        Returns:\n            Node object: The root node of the tree.\n        \"\"\"\n        # the current node\n        class_counts = [np.sum(y == k) for k in self.classes]\n        class_prediction = self.classes[np.argmax(class_counts)]\n        self._node_id += 1\n        node = Node(self._node_id, gini(y), x.shape[0], class_counts, class_prediction)\n\n        # if maximum depth has been reached stop expanding\n        if not (self._max_depth and depth >= self._max_depth):\n            feature, threshold = best_split(x, y, self._min_samples_split)\n            # feature, threshold will be None if the node cannot be split\n            if feature is not None:\n                node.feature, node.threshold = feature, threshold\n                # the left node will get the samples with feature values <= threshold of the split\n                # and the right node the rest\n                permutation_left = np.nonzero(x[:, feature] <= threshold)\n                permutation_right = np.nonzero(x[:, feature] > threshold)\n                node.left = self._build(x[permutation_left], y[permutation_left], depth + 1)\n                node.right = self._build(x[permutation_right], y[permutation_right], depth + 1)\n        return node\n\n    def predict(self, x):\n        \"\"\"Predicts the class of every sample in x.\n\n        Args:\n            x (array-like, shape = [n_samples, n_features]): The input samples. Values must be numeric.\n\n        Returns:\n            ndarray of shape = [n_samples]: The class predictions of samples in x.\n        \"\"\"\n        x = np.copy(x)\n        predictions = x.shape[0]*[0]\n        for i, sample in enumerate(x):\n            node = self.tree\n            while node.left:\n                node = node.left if sample[node.feature] <= node.threshold else node.right\n            predictions[i] = node.class_prediction\n        return np.array(predictions)\n\n    def accuracy(self, x, y):\n        \"\"\"Calculates the mean accuracy on the test set (x, y).\n\n        Args:\n            x (array-like, shape = [n_samples, n_features]): The test samples. Values must be numeric.\n            y (array-like, shape = [n_samples]): The test class labels. Values can be integers or strings.\n\n        Returns:\n            float: The mean accuracy.\n        \"\"\"\n        predictions = self.predict(x)\n        return np.mean(predictions == y)\n\n    def classification_report(self, x, y, plot=False, cmap='YlOrRd'):\n        \"\"\"The classification report consists of the precision, the recall and the F1 scores. It returns a dictionary\n        with the scores and will also display a heatmap if the argument plot is set to True.\n\n        Args:\n            x (array-like, shape = [n_samples, n_features]): The test samples. Values must be numeric.\n            y (array-like, shape = [n_samples]): The test class labels. Values can be integers or strings.\n            plot (bool, default=False): If true, displays heatmap.\n            cmap (string, default='YlOrRd'): The name of the colormap to be used for the heatmap if plot=True.\n\n        Returns:\n            dict: A dictionary that contains the precision, recall and F1 scores for each class.\n        \"\"\"\n        y = np.copy(y)\n        predictions = self.predict(x)\n        report = {}\n        for c in self.classes:\n            tp = sum(np.sum(predictions[i] == c and y[i] == c) for i in range(y.shape[0]))\n            precision = tp / np.sum(predictions == c)\n            recall = tp / np.sum(y == c)\n            f1_score = 2*precision*recall / (precision + recall)\n            report[c] = {'precision': precision, 'recall': recall, 'f1': f1_score}\n        if plot:\n            visualize_classification_report(report, self.classes, cmap)\n        return report\n\n    def feature_importances(self, plot=False, feature_names=None, color='steelblue'):\n        \"\"\"Returns the feature importances. If the argument plot is set to true it will also display them in a plot.\n\n        The feature importance is computed as the normalized total reduction of node gini impurity weighted by the\n        probability of reaching that node.\n\n        Args:\n            plot (bool, default=False): If true, displays the feature importances in a plot.\n            feature_names (array-like, shape = [n_features]): The feature names. Used for the plot.\n            color (string, default='steelblue'): The color to use for the plot.\n\n        Returns:\n            ndarray of shape = [n_features]: The feature importances.\n        \"\"\"\n        self._calc_feature_importances(self.tree)\n        # normalize feature importances to be summed to 1\n        self._feature_importances = self._feature_importances / np.sum(self._feature_importances)\n        if plot:\n            visualize_feature_importances(self._feature_importances, feature_names, color)\n        return self._feature_importances\n\n    def _calc_feature_importances(self, node):\n        \"\"\"Calculates the feature importances.\n\n        Args:\n            node (Node object): The current node. The initial node is the root of the tree.\n        \"\"\"\n        if node.left:\n            node_importance = node.samples / self._n_samples * node.gini - \\\n                              node.left.samples / self._n_samples * node.left.gini - \\\n                              node.right.samples / self._n_samples * node.right.gini\n            self._feature_importances[node.feature] += node_importance\n\n            self._calc_feature_importances(node.left)\n            self._calc_feature_importances(node.right)\n\n    def get_depth(self):\n        \"\"\"Returns the depth of the decision tree.\n        \"\"\"\n        return self._get_depth(self.tree)\n\n    def _get_depth(self, node, current_depth=0, depth=0):\n        \"\"\"Recursively finds the depth of the decision tree.\n\n        Args:\n            node (Node object): The current node. The initial node is the root of the tree.\n            current_depth (int): The depth of the current node.\n            depth (int): The maximum depth found.\n        \"\"\"\n        if depth < current_depth:\n            depth = current_depth\n        if node.left:\n            depth = self._get_depth(node.left, current_depth + 1, depth)\n            depth = self._get_depth(node.right, current_depth + 1, depth)\n        return depth\n\n    def get_n_leaves(self):\n        \"\"\"Returns the number of leaves of the decision tree.\n        \"\"\"\n        return self._get_n_leaves(self.tree)\n\n    def _get_n_leaves(self, node, leaves=0):\n        \"\"\"Recursively finds the number of leaves of the decision tree.\n\n        Args:\n            node (Node object): The current node. The initial node is the root of the tree.\n            leaves (int): The number of leaves.\n        \"\"\"\n        if node.left is None:\n            leaves += 1\n        else:\n            leaves = self._get_n_leaves(node.left, leaves)\n            leaves = self._get_n_leaves(node.right, leaves)\n        return leaves\n\n    def print_tree(self, max_depth=None):\n        \"\"\"Prints the information of the tree's nodes.\n\n        Args:\n            max_depth (int or None, default=None): The max depth to print.\n        \"\"\"\n        self._print_tree(self.tree, max_depth)\n\n    def _print_tree(self, node, max_depth, depth=0):\n        \"\"\"Recursively prints the information of the tree's nodes.\n\n        Args:\n            node (Node object): The current node. The initial node is the root of the tree.\n            max_depth (int or None): The max depth to print.\n            depth (int): The current depth.\n        \"\"\"\n        if max_depth and depth > max_depth:\n            return\n        print(\"Depth:\", depth)\n        if node.left is None:\n            print(\"node #\" + str(node.node_id), \"| gini =\", \"%.3f\" % round(node.gini, 3), \"| samples =\", node.samples,\n                  \"| value =\", node.class_counts, \"| class =\", node.class_prediction)\n        else:\n            print(\"node #\" + str(node.node_id), \"| X\" + str(node.feature), \"<=\", node.threshold,\n                  \"| gini =\", \"%.3f\" % round(node.gini, 3), \"| samples =\", node.samples, \"| value =\", node.class_counts,\n                  \"| class =\", node.class_prediction)\n            self._print_tree(node.left, max_depth, depth + 1)\n            self._print_tree(node.right, max_depth, depth + 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Node"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Node:\n    \"\"\"Tree node representation.\n\n    Args:\n        node_id (int): The unique id of the node.\n        gini (float): The gini impurity of the node.\n        samples (int): The number of samples of the node.\n        class_counts (array, shape[n_classes]): The number of samples corresponding to each class.\n        class_prediction (int or string): The predicted class of the node, which is the class with the most samples.\n        feature (int): The feature index of the split.\n        threshold (float): The threshold value of the split.\n        left (Node object): The left child of the node.\n        right (Node object): The right child of the node.\n    \"\"\"\n    def __init__(self, node_id, gini, samples, class_counts, class_prediction, feature=None, threshold=None, left=None,\n                 right=None):\n        self.node_id = node_id\n        self.gini = gini\n        self.samples = samples\n        self.class_counts = class_counts\n        self.class_prediction = class_prediction\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(y):\n    \"\"\"Calculates the gini impurity of a node.\n\n    Args:\n        y (array-like, shape[n_samples]): The class labels of a node's samples.\n\n    Returns:\n        float: The gini impurity.\n    \"\"\"\n    classes, counts = np.unique(y, return_counts=True)\n    return 1 - np.sum((counts / y.shape[0]) ** 2)\n\n\ndef best_split(x, y, min_samples_split):\n    \"\"\"Finds the best split point of a node.\n\n    Checks all the possible split points for every feature and returns the feature index and threshold value of the\n    split that minimizes the gini impurity.\n\n    Args:\n        x (array-like, shape = [n_samples, n_features]): The node's samples.\n        y (array-like, shape = [n_samples]): The node's class labels.\n        min_samples_split (int): The minimum samples a node must have to consider splitting it.\n\n    Returns:\n        best_feature (int): The feature index of the best split.\n        best_threshold (float): The threshold value of the best split.\n    \"\"\"\n    n = y.shape[0]\n    best_feature, best_threshold = None, None\n    best_gini = gini(y)\n\n    # stop splitting if the node has less than the minimum samples or if all the samples belong to the same class\n    if n < min_samples_split or best_gini == 0:\n        return best_feature, best_threshold\n\n    for feature in range(x.shape[1]):\n        permutation = x[:, feature].argsort()\n        x_sorted, y_sorted = x[permutation], y[permutation]\n\n        # can't split between same values so skip\n        for i in range(1, n):\n            if x_sorted[i, feature] == x_sorted[i - 1, feature]:\n                continue\n\n            # check if the split impurity is minimum\n            gini_split = (i * gini(y_sorted[:i]) + (n - i) * gini(y_sorted[i:])) / n\n            if gini_split < best_gini:\n                best_gini = gini_split\n                best_feature = feature\n                best_threshold = (x_sorted[i, feature] + x_sorted[i - 1, feature]) / 2\n    return best_feature, best_threshold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_classification_report(report, classes, cmap):\n    \"\"\"Creates a heatmap for the precision, recall and f1 scores of the classes.\n\n    Args:\n        report (dict): A dictionary with the precision, recall and f1 scores of each class.\n        classes (array): The class labels.\n        cmap: (string): The name of the colormap to be used for the heatmap.\n    \"\"\"\n    values = np.array([[report[c]['precision'], report[c]['recall'], report[c]['f1']] for c in report])\n\n    # create the heatmap\n    fig, ax = plt.subplots()\n    pcolor = ax.pcolor(values, edgecolors='white', linewidths=0.8, cmap=cmap, vmin=0, vmax=1)\n    ax.set_xticks(np.arange(values.shape[1]) + 0.5)\n    ax.set_yticks(np.arange(values.shape[0]) + 0.5)\n    ax.set_xticklabels(['Precision', 'Recall', 'F1'])\n    ax.set_yticklabels(classes)\n\n    # add values to the heatmap\n    pcolor.update_scalarmappable()\n    for path, color, value in zip(pcolor.get_paths(), pcolor.get_facecolors(), pcolor.get_array()):\n        x, y = path.vertices[:-2, :].mean(0)\n        # set text color according to background color for better visibility\n        color = (0.0, 0.0, 0.0) if np.all(color[:3] > 0.5) else (1.0, 1.0, 1.0)\n        ax.text(x, y, \"%.3f\" % value, ha=\"center\", va=\"center\", color=color)\n\n    cbar = plt.colorbar(pcolor, values=np.arange(0.0, 1.1, 0.1))\n    cbar.outline.set_edgecolor('lightgray')\n    cbar.outline.set_linewidth(1.2)\n    plt.setp(ax.spines.values(), color='silver')\n    plt.title(\"Classification Report\")\n\n\ndef visualize_feature_importances(feature_importances, feature_names, color):\n    \"\"\"Creates a bar chart for the feature importances.\n\n    Args:\n        feature_importances (array-like, shape = [n_features]): The feature importances.\n        feature_names (array-like, shape = [n_features]): The feature names.\n        color (string): The color of the bars.\n    \"\"\"\n    n_features = feature_importances.shape[0]\n    sorted_indices = np.argsort(feature_importances)\n\n    fig, ax = plt.subplots()\n    plt.barh(range(n_features), feature_importances[sorted_indices], color=color)\n    if feature_names is None:\n        plt.yticks(range(n_features), sorted_indices)\n    else:\n        plt.yticks(range(n_features), feature_names)\n\n    ax.xaxis.grid(color='lightgray')\n    ax.set_axisbelow(True)\n    plt.setp(ax.spines.values(), color='lightgray')\n    plt.title(\"Feature Importances\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test decision tree with car evaluation data set."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# load data\ncar_data = pd.read_csv('/kaggle/input/car-evaluation-data-set/car_evaluation.csv')\ny = car_data.iloc[:, -1]\nfeature_names = ['buying', 'maint', 'doors', 'persons', 'lug_boots', 'safety']\nclass_names = y.unique()\n\n# label encoding\nle = preprocessing.LabelEncoder()\nfor i in range(len(car_data.columns) - 1):\n    car_data.iloc[:, i] = le.fit_transform(car_data.iloc[:, i])\nX = car_data.iloc[:, :-1]\n    \n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\ndt = DecisionTree(max_depth=None, min_samples_split=2)\ndt.build(X_train, y_train)\n\nprediction = dt.predict(X_test)\naccuracy = dt.accuracy(X_test, y_test)\nreport = dt.classification_report(X_test, y_test, plot=True)\ndepth = dt.get_depth()\nleaves = dt.get_n_leaves()\nfeature_importances = dt.feature_importances(plot=True, feature_names=feature_names)\nprint('Accuracy:', accuracy)\nprint('Depth:', depth)\nprint('Leaves:', leaves)\nprint('Precision, recall, f1:', report)\nprint('Feature importances:', feature_importances)\nprint('First 2 layers of the tree:')\ndt.print_tree(max_depth=2)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}