{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network + XGBoost + Gaussian Naive Bayers\n\nThis notebook hybrids CNN and XGBoost algortihms in a noval manner \n\nNote :- This work is done under research basis but I was able topull of above 90% accuracy with this mehtod for Kannada MNIST dataset.\n### upvote if you like\n\nThank you for:Yominma for helping me with his [notebook](http://https://www.kaggle.com/yonminma/keras-easy-with-0-9892-score)"},{"metadata":{},"cell_type":"markdown","source":"# CNN\n\nCNN was one of the giant steps towards perfect Computer Vision/AI. It was initiated by [Yann LeCun, Patrick Haffner, LÃ©eon Bottou, and Yoshua Bengio](http://http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Initial Images where fed to Deep Neural Networks directly. This caused many problems like high computational intensity due to the high dimensionality of input images (256x256x3 image will have 196608 number of dimensions/inputs). also, DNN was less accurate. Consider the image given below.\n![](https://res.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png)\n\nThis image represents full steps of CNN.\n1. ** Convolution + RELU**\nConvolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. This is related to a form of mathematical convolution. It should be noted that the matrix operation being performed - convolution - is not traditional matrix multiplication. A kernel, convolution matrix, or mask is a small matrix. It is used for blurring, sharpening, embossing, edge detection, and more. This is accomplished by doing a convolution between a kernel and an image.\nRELU-Rectified Linear Unit is an activation function which is used along with it and its simply an operation which converts -ve values into zeros and keeps +ve values as it is\n[](https://cdn.tinymind.com/static/img/learn/relu.png)\n2. ** Pooling**\nPooling is an operation of feature extraction and dimensionality reduction. It will extract the features from the given images region of interests by doing non-linear down-sampling.\nIt can be max Pool/min pool. Example of a max pool is given below\n![](http://cs231n.github.io/assets/cnn/maxpool.jpeg)\n\n3. We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.\n\n4. Activation function is used to introduce non-leniarity to the model so that we can realize complex realworld problem. ReLU (Rectified Leniar Unit) is a HIghly used activation function, we use its advanced form leaky relu, for more detals :\nhttps://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e\n\n\n5. ** Fully Connected Layer / Dense Layer**\n   This is a set of a layer of fully connected DNN to process and get trained weights.This layer is followed by the output layer"},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\n\nThis is one of the advanced form of machine learning it comes under the category of ensemble learning. While it gives us amazing accuracy for robust machine learning datasets, but they under perform when it comes to image dataset. It is an ensemble of decision tree algorithms, for more details:https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"},{"metadata":{},"cell_type":"markdown","source":"# GNB\nIn Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. The Gaussian Naive Bayes is one classifier model. Beside the Gaussian Naive Bayes there are also existing the Multinomial naive Bayes and the Bernoulli naive Bayes. I picked the Gaussian Naive Bayes because it is the simplest and the most popular one, for more details: https://medium.com/@LSchultebraucks/gaussian-naive-bayes-19156306079b"},{"metadata":{},"cell_type":"markdown","source":"# Combination \n![](https://storage.googleapis.com/kagglesdsdata/datasets/447361/846442/Untitled-1.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1576765070&Signature=dygGhBxGOI%2BvesnEf%2FnMimjpTM0lAzWEHbr9yHFjGYB86TaIfxQThwzfehifhY6pWPJxEd92HL9U8gAB19IEshUO4RiguvBcEHcJB1Mn46LY5ZQRMf4bJosXzzPia0dXWC6E6sasPgN5f6M0%2FeRc73E7hBX1yg1tAFrlDGss5Dm5iEF97d50plS8ZLcAF6zaGfa30KcB9rhmmEuWOvUbc0YkvjM%2FXAq%2FEnHO%2FEgEwUAp9ArR9cA%2Fe9I1STzlTsUWqeziY0fH9DuIQF3gyHVdKh%2FqufBKGtR2IHDMsRS1HLaCTZRb6%2BONuIxJAyErq%2FvxWNv%2B9VVJ6mVHdN8r2de8wA%3D%3D)\n\n\nWe will Perform CNN and by extracting intermediate layer information we will impliment XGBoost and GNB and we will perform bagging of those algorithm (where each algorithm gets 1 vote to chose what is the right classification)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport os\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = data[:]\nval_data = data[53000:]\ntrain_label = np.float32(train_data.label)\nval_label = np.float32(val_data.label)\ntrain_image = np.float32(train_data[train_data.columns[1:]])\nval_image = np.float32(val_data[val_data.columns[1:]])\ntest_image = np.float32(test_data[test_data.columns[1:]])\nprint('train shape: %s'%str(train_data.shape))\nprint('val shape: %s'%str(val_data.shape))\nprint('train_label shape: %s'%str(train_label.shape))\nprint('val_label shape: %s'%str(val_label.shape))\nprint('train_image shape: %s'%str(train_image.shape))\nprint('val_image shape: %s'%str(val_image.shape))\nprint('test_image shape: %s'%str(test_image.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_image[13].reshape(28,28))\nplt.show()\nprint(train_image[13].shape)\n\ntrain_image = train_image/255.0\nval_image = val_image/255.0\ntest_image = test_image/255.0\n\ntrain_image = train_image.reshape(train_image.shape[0],28,28,1)\nval_image = val_image.reshape(val_image.shape[0],28,28,1)\ntest_image = test_image.reshape(test_image.shape[0],28,28,1)\nprint('train_image shape: %s'%str(train_image.shape))\n\nprint('train_image shape: %s'%str(train_image.shape))\nprint('val_image shape: %s'%str(val_image.shape))\n\ntrain_label1 = train_label\nval_label1 = val_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False,categories='auto')\nyy = [[0],[1],[2],[3],[4],[5],[6],[7],[8],[9]]\nencoder.fit(yy)\n# transform\ntrain_label = train_label.reshape(-1,1)\nval_label = val_label.reshape(-1,1)\n\n\ntrain_label = encoder.transform(train_label)\nval_label = encoder.transform(val_label)\n\n\n\nprint('train_label shape: %s'%str(train_label.shape))\nprint('val_label shape: %s'%str(val_label.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import LeakyReLU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n# input: 28x28 images with 1 channels -> (28, 28, 1) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1),padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(128, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(128, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(256, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(256, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu', name='my_dense'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\n#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=sgd)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nlayer_name='my_dense'\nintermediate_layer_model = Model(inputs=model.input,\n                                 outputs=model.get_layer(layer_name).output)\n\nintermediate_layer_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range = 10,\n    horizontal_flip = False,\n    zoom_range = 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam, Adadelta, RMSprop\n\nmodel.compile(loss='categorical_crossentropy',optimizer=Adadelta(),metrics=['accuracy'])\n\ndatagen.fit(train_image)\n\n# training\nhistory = model.fit_generator(datagen.flow(train_image,train_label, batch_size=32),\n                              epochs = 75,\n                              shuffle=True,\n                              validation_data = (val_image,val_label),\n                              verbose = 1,\n                              steps_per_epoch=train_image.shape[0] // 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intermediate_output = intermediate_layer_model.predict(train_image) \nintermediate_output = pd.DataFrame(data=intermediate_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data = intermediate_output[53000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_cnn = model.predict(test_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intermediate_test_output = intermediate_layer_model.predict(test_image)\nintermediate_test_output = pd.DataFrame(data=intermediate_test_output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgbmodel = XGBClassifier(objective='multi:softprob', \n                      num_class= 10)\nxgbmodel.fit(intermediate_output, train_label1)\nxgbmodel.score(val_data, val_label1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nintermediate_layer_model.predict(test_image)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_xgb = xgbmodel.predict(intermediate_test_output)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \ngnbmodel = GaussianNB().fit(intermediate_output, train_label1) \n\nsubmission_gnb = gnbmodel.predict(intermediate_test_output) \ngnbmodel.score(val_data, val_label1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_cnn = submission_cnn.astype(int)\nsubmission_xgb = submission_xgb.astype(int)\nsubmission_gnb = submission_gnb.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_cnn\nlabel = np.argmax(submission_cnn,1)\nid_ = np.arange(0,label.shape[0])\nlabel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_gnb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub = submission_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,submission_xgb.shape[0]):\n    if (label[i] == submission_xgb[i]):\n        final_sub[i] = submission_xgb[i]\n    elif (label[i] == submission_gnb[i]):\n        final_sub[i] = submission_gnb[i]\n    elif (submission_gnb[i] == submission_xgb[i]):\n        final_sub[i] = submission_gnb[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsave = pd.DataFrame({'id':test_data.id,'label':final_sub})\nprint(save.head(10))\nsave.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}