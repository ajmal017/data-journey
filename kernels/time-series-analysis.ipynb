{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing Librabries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n# warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Path = '../input/competitive-data-science-predict-future-sales/'\nshops = pd.read_csv(Path +'shops.csv')\nsales_train = pd.read_csv(Path+'sales_train.csv',parse_dates=True)\nitem_categories = pd.read_csv(Path+'item_categories.csv')\ntest = pd.read_csv(Path+'test.csv')\nitem = pd.read_csv(Path+'items.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data fields**\n\nID - an Id that represents a (Shop, Item) tuple within the test set\n\nshop_id - unique identifier of a shop\n\nitem_id - unique identifier of a product\n\nitem_category_id - unique identifier of item category\n\nitem_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n\nitem_price - current price of an item\n\ndate - date in format dd/mm/yyyy\n\ndate_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n\nitem_name - name of item\n\nshop_name - name of shop\n\nitem_category_name - name of item category"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(shops.shape,sales_train.shape,item_categories.shape , test.shape,item.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_cat = item.groupby(['item_category_id']).size().sort_values(ascending=False).head(20).plot.bar()\nplt.xlabel('Items per category')\nplt.ylabel('No. of Times')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_sale = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\n#total_sale.head()\nplt.figure(figsize=(8,8))\nplt.title('Total sales of company')\nplt.xlabel('Time (January 2013 is 0, February 2013 is 1,..., October 2015 is 33)')\nplt.ylabel('sales')\nplt.plot(total_sale)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A rolling analysis of a time series model is often used to assess the model’s stability over time. When analyzing financial time series data using a statistical model, a key assumption is that the parameters of the model are constant over time. However, the economic environment often changes considerably, and it may not be reasonable to assume that a model’s parameters are constant. A common technique to assess the constancy of a model’s parameters is to compute parameter estimates over a rolling window of a fixed size through the sample. If the parameters are truly constant over the entire sample, then the estimates over the rolling windows should not be too different. If the parameters change at some point during the sample, then the rolling estimates should capture this instability."},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking timeseries stationarity \n#1. plotting rolling statistics curve\n#2. dickey fuller test \nplt.figure(figsize=(16,6))\nplt.plot(total_sale, color='blue',label='Original')\nplt.plot(total_sale.rolling(window= 12,center= False).mean(),label='Rolling Mean')\nplt.plot(total_sale.rolling(window=12,center= False).std(),label='Rolling std')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#formatting the date column correctly\n#sales_train.date=sales_train.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n\nsales_train['date'] = pd.to_datetime(sales_train['date'], format=\"%d.%m.%Y\")\nsales_train['DayOfWeekNum'] = sales_train['date'].dt.dayofweek\nsales_train['DayOfWeek'] = sales_train['date'].dt.weekday_name\nsales_train['MonthDayNum'] = sales_train['date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking Trends, seasionality and residuals **"},{"metadata":{},"cell_type":"markdown","source":"Time series datasets may contain trends and seasonality, which may need to be removed prior to modeling.\n\nTrends can result in a varying mean over time, whereas seasonality can result in a changing variance over time,"},{"metadata":{},"cell_type":"markdown","source":"**Additive or multiplicative?**\n\nIt’s important to understand what the difference between a multiplicative time series and an additive one before we go any further.\n\nThere are three components to a time series:\n\n– trend how things are overall changing\n\n– seasonality how things change within a given period e.g. a year, month, week, day\n\n– error/residual/irregular activity not explained by the trend or the seasonal value\n\nHow these three components interact determines the difference between a multiplicative and an additive time series.\n\nIn a multiplicative time series, the components multiply together to make the time series. If you have an increasing trend, the amplitude of seasonal activity increases. Everything becomes more exaggerated. This is common when you’re looking at web traffic.\n\nIn an additive time series, the components add together to make the time series. If you have an increasing trend, you still see roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the absolute value is growing but changes stay relative."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\ntse = sm.tsa.seasonal_decompose(total_sale.values,freq=12,model='multiplicative')\ntse.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we assume an additive model, then we can write\n\n**yt=St+Tt+Et**\n\nwhere yt is the data at period t, St is the seasonal component at period t, Tt is the trend-cycle component at period tt and Et is the remainder (or irregular or error) component at period t Similarly for Multiplicative model,\n\n**yt=St x Tt x Et**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tse = sm.tsa.seasonal_decompose(total_sale.values,freq=12,model='additive')\ntse.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stationarity **"},{"metadata":{},"cell_type":"markdown","source":"**Stationary Time Series:**\n\ndata does not have any upward or downward trend or seasonal effects. Mean or variance are consistent over time.\n"},{"metadata":{},"cell_type":"markdown","source":"![](http://miro.medium.com/max/454/1*jNeo1bAgBOS_FvyUhDi-0g.png)"},{"metadata":{},"cell_type":"markdown","source":"**Non-Stationary Time Series:**\n\ndata show trends, seasonal effects, and other structures depend on time. Forecasting performance is dependent on the time of observation. Mean and variance change over time and a drift in the model is captured."},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/411/1*-iOconMEaLW5ttxhHCdKBw.png)"},{"metadata":{},"cell_type":"markdown","source":"**Augmented Dickey-Fuller test (ADF)**\n\nADF tests the null hypothesis that a unit root is present in time series sample. ADF statistic is a negative number and more negative it is the stronger the rejection of the hypothesis that there is a unit root.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\n**Null Hypotehsis (H0): **\n\nIf accepted, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n\n**Alternate Hypothesis (H1):** \n\nThe null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary.\n\n**p-value > 0.05: **\n\nFail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n\n**p-value <= 0.05: **\n\nReject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\n**means if null hypothesis(H0) accepted, p>0.05 then series is not stationary **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(total_sale)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that statistic is greater than all critical values ,means we can accept null hypothesis from both statement we can conclude that series is not stationary . we have convert the series into stationary series. \n"},{"metadata":{},"cell_type":"markdown","source":">** Difference Transform**\n\nDifferencing is a method of transforming a time series dataset."},{"metadata":{},"cell_type":"markdown","source":"Differencing is performed by subtracting the previous observation from the current observation.\n\n**difference(t) = observation(t) - observation(t-1)**\n\nInverting the process is required when a prediction must be converted back into the original scale.\n\nThis process can be reversed by adding the observation at the prior time step to the difference value.\n\n**inverted(t) = differenced(t) + observation(t-1)**\n\nIn this way, a series of differences and inverted differences can be calculated.\n\n**Lag Difference**\n\nTaking the difference between consecutive observations is called a lag-1 difference.\n\nThe lag difference can be adjusted to suit the specific temporal structure.\n\nFor time series with a seasonal component, the lag may be expected to be the period (width) of the seasonality.\n\n**Difference Order**\n\nSome temporal structure may still exist after performing a differencing operation, such as in the case of a nonlinear trend.\n\nAs such, the process of differencing can be repeated more than once until all temporal dependence has been removed.\n\nThe number of times that differencing is performed is called the difference order."},{"metadata":{"trusted":true},"cell_type":"code","source":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_ts1=difference(total_sale,12) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_ts=difference(total_sale)\n\nfig, axs = plt.subplots(3,figsize=(16,16))\naxs[0].plot(total_sale)\naxs[0].set_title('original')\n\naxs[1].plot(new_ts)\naxs[1].set_title('After De-trend')\n\naxs[2].plot(new_ts1)# assuming the seasonality is 12 months long\naxs[2].set_title('After De-serialization')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series.\n\nWe can easily get back the original series using the inverse transform function that we have defined above."},{"metadata":{},"cell_type":"markdown","source":"** Introduction to ARMA Time Series Modeling**"},{"metadata":{},"cell_type":"markdown","source":"ARMA models are commonly used in time series modeling. In ARMA model, AR stands for auto-regression and MA stands for moving average. If these words sound intimidating to you, worry not – I’ll simplify these concepts in next few minutes for you!\n\nWe will now develop a knack for these terms and understand the characteristics associated with these models. But before we start,** you should remember, AR or MA are not applicable on non-stationary series**\n\nA pure Auto Regressive (AR only) model is one where Yt depends only on its own lags. That is, Yt is a function of the ‘lags of Yt’.\n\n![](https://www.machinelearningplus.com/wp-content/uploads/2019/02/Equation-1-min.png)\n\nwhere, $Y{t-1}$ is the lag1 of the series, $\\beta1$ is the coefficient of lag1 that the model estimates and $\\alpha$ is the intercept term, also estimated by the model.\n\nLikewise a pure Moving Average (MA only) model is one where Yt depends only on the lagged forecast errors.\n\n![](https://www.machinelearningplus.com/wp-content/uploads/2019/02/Equation-2-min.png)\n\nAn ARIMA model is one where the time series was differenced at least once to make it stationary and you combine the AR and the MA terms. So the equation becomes:\n\n![](https://www.machinelearningplus.com/wp-content/uploads/2019/02/Equation-4-min-1024x91.png)\n\n\n\nARIMA model in words:\n\n\nPredicted Yt = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)\n\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n\nwhere,\n\n**1. Number of AR (Auto-Regressive) terms (p): **AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n\n**2. Number of MA (Moving Average) terms (q):**   MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\n**3. Number of Differences (d):**  These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results"},{"metadata":{},"cell_type":"markdown","source":"An importance concern here is how to determine the value of ‘p’ and ‘q’. We use two plots to determine these numbers. Lets discuss them first.\n\n**Autocorrelation Function (ACF): **\n\nIt is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n\n**Partial Autocorrelation Function (PACF):**\n\nThis measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(total_sale, nlags=20)\nlag_pacf = pacf(total_sale, nlags=20, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(total_sale)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(total_sale)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(total_sale)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(total_sale)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the ‘p’ and ‘q’ values as:\n\np – The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.\n\nq – The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=2.\n\nNow, lets make 3 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.\n\nWe need to load the ARIMA model first:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The p,d,q values can be specified using the order argument of ARIMA which take a tuple (p,d,q). Let model the 3 cases:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(total_sale, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(total_sale)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-total_sale)**2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}