{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Overview\n- We have a dataset from the moview review website RottenTomatoes.com.\n- The dataset includes written reviews on movies, the names of each critic, review date, and a verdict of whether of not a movie was consider \"Fresh\" (positive review) or \"Rotten\" (negative review).\n- The goal of this project is to use the text in each review to create a model to predict whether or not a review is considered \"Fresh\" or \"Rotten\" based solely on the information extracted from the text review alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport os\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading in dataset\ndf = pd.read_csv('../input/rotten-tomatoes/reviews.tsv', sep='\\t', encoding = 'ISO-8859-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n- For our movie review classification, we will stick with the Rotten Tomoatoes benchmark of \"Fresh\" vs \"Rotten\".\n- This is a simpler task than predicting a rating as many of the critics appear to use their own metrics (e.g. numeric rating system out of 4, 5 or out of 10. Other critics use a letter grade system."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Way too many types of reviews from critics with each reviewer using their own set of review rating system\nprint('List of Reviews:')\nprint(df['rating'].unique())\nprint('\\n')\nprint('Number of unique reviews:')\nprint(df['rating'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll stick with Rotten Tomatoes' final review classification of \"Fresh\" vs. \"Rotten\" when training our model\ndf['fresh'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['fresh'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of \"Fresh\" vs \"Rotten\" reviews are roughly balanced. \nsns.countplot(df['fresh'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we cannot work with missing data or find a viable way to replace missing text reviews, we will drop these missings rows under reviews.\ndf = df.dropna(subset=['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fresh = df[['fresh', 'review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for reviews with no text in review\nblank_reviews = []\n\n# (index, label, review text)\nfor i, label, review in df_fresh.itertuples():\n    if type(review) == str:\n        if review.isspace():\n            blank_reviews.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All remaining reviews contain text\nblank_reviews","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Addining in a new feature to see if there is any correlation to the length of the review to the fresh rating.\ndf_fresh['review length'] = df_fresh['review'].apply(lambda review: len(review))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fresh.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(df_fresh[df_fresh['fresh']=='fresh']['review length'],bins=bins,alpha=0.5)\nplt.hist(df_fresh[df_fresh['fresh']=='rotten']['review length'],bins=bins,alpha=0.5)\nplt.legend(('fresh','rotten'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Not a clear trend to see if the length of reviews has any relation to the movie review.\n- The distribution of review length looks pretty similar between \"Fresh\" movies and \"Rotten\" movies."},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\n- Feature extraction of the text reviews was performed using TfidfVectorizer.\n- The classification models evaluated include:\n- LinearSVC\n- Logistic Regression Model\n- XGBoost\n- Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into training and testing datasets\nfrom sklearn.model_selection import train_test_split\n\nX = df_fresh['review']\ny = df_fresh['fresh']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear SVC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a simple pipeline to preprocess text data\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\ntext_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),\n                    ('clf', LinearSVC())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting and generating predictions\ntext_clf_svc.fit(X_train, y_train)\ny_pred_svc = text_clf_svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(classification_report(y_test, y_pred_svc))\nprint(confusion_matrix(y_test, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Building pipeline\ntext_clf_lr = Pipeline([('tfidf', TfidfVectorizer()),\n                    ('clf', LogisticRegression())])\n# Fitting and generating predictions\ntext_clf_lr.fit(X_train, y_train)\ny_pred_lr = text_clf_lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_lr))\nprint(confusion_matrix(y_test, y_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n# Building pipeline\ntext_clf_xgb = Pipeline([('tfidf', TfidfVectorizer()),\n                    ('clf', XGBClassifier())])\n# Fitting and generating predictions\ntext_clf_xgb.fit(X_train, y_train)\ny_pred_xgb = text_clf_xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_xgb))\nprint(confusion_matrix(y_test, y_pred_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Building pipeline\ntext_clf_rf = Pipeline([('tfidf', TfidfVectorizer()),\n                    ('clf', RandomForestClassifier())])\n# Fitting and generating predictions\ntext_clf_rf.fit(X_train, y_train)\ny_pred_rf = text_clf_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_rf))\nprint(confusion_matrix(y_test, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparisons"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance = [accuracy_score(y_test, y_pred_svc),accuracy_score(y_test, y_pred_lr),accuracy_score(y_test, y_pred_xgb),accuracy_score(y_test, y_pred_rf)]\nmodels = ['Linear SVC', 'Logistic Regression', 'XGBoost', 'Random Forest']\ndf_model = pd.DataFrame(model_performance, columns=['Accuracy'])\ndf_model['Model'] = models\ndf_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.ylim(0.5,0.8)\nsns.barplot(x='Model', y='Accuracy', data=df_model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The top two performers were the linear SVC and logistic regression models.\n- Ensemble tree based models don't appear to be that great.\n- Surprisingly the XGBoost model performed worse than the normal random forest model, and the worst out of the four models."},{"metadata":{},"cell_type":"markdown","source":"# Sample Predictions\n- We will use the linear SVC model for the sample predictions.\n- Randomly selected reviews from the testing data will be used for these predictions."},{"metadata":{},"cell_type":"markdown","source":"**Sample Prediction 1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nrand_sample_1 = int(np.random.randint(0, len(X_test), size=1))\nlist(X_test)[rand_sample_1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_1 = text_clf_svc.predict([list(X_test)[rand_sample_1]])\ny_pred_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['review'] == 'As a work of cinema, The Passion of the Christ possesses a majestic beauty within its horror, one that comes most effectively through a tiny, solitary teardrop.']['fresh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Model Predition: 'fresh'\n- True Result: 'fresh'"},{"metadata":{},"cell_type":"markdown","source":"**Sample Prediction 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(43)\nrand_sample_2 = int(np.random.randint(0, len(X_test), size=1))\nlist(X_test)[rand_sample_2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2 = text_clf_svc.predict([list(X_test)[rand_sample_2]])\ny_pred_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['review'] == 'A character-driven dramedy with equal parts humor and heart, Safety Not Guaranteed is a magical film about the human spirit whose charm is impossible to ignore.']['fresh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Model Predition: 'fresh'\n- True Result: 'fresh'"},{"metadata":{},"cell_type":"markdown","source":"**Sample Prediction 3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(44)\nrand_sample_3 = int(np.random.randint(0, len(X_test), size=1))\nlist(X_test)[rand_sample_3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_3 = text_clf_svc.predict([list(X_test)[rand_sample_3]])\ny_pred_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['review'] == 'My mother is going to love this movie. ']['fresh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Model Predition: 'rotten'\n- True Result: 'rotten'"},{"metadata":{},"cell_type":"markdown","source":"**Some notes on these preditions:** <br>\n- After surveying the text reviews for these sample predictions, it seems reasonable to predict on our own that the first two reviews would be given a \"Fresh\" rating given how the reviews were written.\n- The third review also looked like it had a positive sentiment to it using the word \"love\". However, the model predicted that this review would be \"Rotten\".\n- Interestingly, the actual review was indeed \"Rotten\", perhaps this review was sarcastic and the model appears to have understood this after feature extraction of the text."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}