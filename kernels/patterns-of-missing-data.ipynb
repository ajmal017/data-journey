{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"![app](https://i.imgur.com/jKtMJgg.jpg)\n\nThe missing data within a dataset can often provide insight into the issue at hand. We can look at the structure of the missing values - which features are affected, which records are affected, and differences between groups. We can also use the missing data as a feature itself by counting missing values or transforming them. In this report I explore some patterns and suggest one way to improve your predictive.\n\n## Patterns\nFirst let's look at the overall pattern of missing data. The [missingno](https://github.com/ResidentMario/missingno) package by [Aleksey Bilogur](https://www.kaggle.com/residentmario) is the perfect tool here. Looking at a sample of data for all columns we see a group of columns where the missing values appear correlated.\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport missingno as msno\n\ntrain = pd.read_csv('../input/application_train.csv')\nmsno.matrix(train.sample(500), inline=True, sparkline=True, figsize=(20,10), sort=None, color=(0.25, 0.45, 0.6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e549fce8fd5a9da016b2d40a848a0082bfc6603a"},"cell_type":"markdown","source":"Zooming in on the middle columns we see they deal mostly with information about the building where the client lives. It appears there are many applicants who leave blank the information for their housing. We can think about why that might be the case or how it might inform our model.\n\nI'll sort the data this time to better see the proportions."},{"metadata":{"trusted":true,"_uuid":"2c0e120e12d5b8dd33b7467b7183a64d61f7d9ca","_kg_hide-input":true},"cell_type":"code","source":"msno.matrix(train.iloc[0:100, 40:94], inline=True, sparkline=True, figsize=(20,10), sort='ascending', fontsize=12, labels=True, color=(0.25, 0.45, 0.6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b04c78b33236b79b5ca6ecc6f0351cf3a0d2763"},"cell_type":"markdown","source":"The dendrogram view shows how missing values are related across columns by using hierarchical clustering. Pretty cool! "},{"metadata":{"trusted":true,"_uuid":"353d36bee5f5e4accd6251862696bf73fae4da84","scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"msno.dendrogram(train, inline=True, fontsize=12, figsize=(20,30))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6598dfeef094a75a2d7604a57cbc83e6dc01ca32"},"cell_type":"markdown","source":"## Comparison of Completed Applications\n\nWith an idea of the overall picture, let's now focus on the large group of applications with missing house data. Is there a difference in mean default rates between those with house information and those without?"},{"metadata":{"trusted":true,"_uuid":"65ae2e0cdc8646b781b15d96f2e37afe883c677c","_kg_hide-input":true},"cell_type":"code","source":"train['incomplete'] = 1\ntrain.loc[train.isnull().sum(axis=1) < 35, 'incomplete'] = 0\n\nmean_c = np.mean(train.loc[train['incomplete'] == 0, 'TARGET'].values)\nmean_i = np.mean(train.loc[train['incomplete'] == 1, 'TARGET'].values)\nprint('default ratio for more complete: {:.1%} \\ndefault ratio for less complete: {:.1%}'.format(mean_c, mean_i))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec908acf2e97ca9130d22044fe686c6eeb413d23"},"cell_type":"markdown","source":"There appears to be a difference. Viewed one way, borrowers with incomplete applications are ~30% more likely to default. You may want to include this information in your model. I found it helpful to add a binary feature called 'no_housing_info'. The application is flagged if it has more than 45 blanks. You could also create three classes to account for the applications with some housing data (which may denote apartment dwellers). \n\n\n## Statistical Significance\nTo be thorough, I looked at statistical significance of the difference in default rates between groups. I used a [G-test](https://en.wikipedia.org/wiki/G-test) which is similar to Pearson's chi-squared test. Either one should work in this case, but I generally prefer the G-test."},{"metadata":{"trusted":true,"_uuid":"182bb463085a97f5d828b9cbaa58f96ddf8e5fc0","_kg_hide-input":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\nprops = pd.crosstab(train.incomplete, train.TARGET)\nc = chi2_contingency(props, lambda_=\"log-likelihood\")\nprint(props, \"\\n p-value= \", c[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c97488cda8a681483f59b4456ab6835d75f43e6f"},"cell_type":"markdown","source":"\"If p is low, the null must go.\" The p-value here is 1e-114 which is pretty much 0. So we can reject the null hypothesis with only a small probability of [Type 1 error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). In other words, the difference in default ratios between the two groups is not due to random chance. \n\nGood luck!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}