digraph { 
"set_random_seed[0]" -> "2(0)" [label = "set_random_seed"]
"seed[0]" -> "1(0)" [label = "seed"]
"warnings(0)$0" -> "warnings(0)" [label = "filterwarnings"]
"warnings(0)$0" -> "ignore(0)" [label = "filterwarnings"]
"warnings(0)$1" -> "warnings(0)$0" [label = "simplefilter"]
"curr_dir(0)$0" -> "../input/(0)" [label = "eq"]
"all_headlines(0)$0" -> "[](0)" [label = "eq"]
"filename(0)" -> "os(0)" [label = "Iter"]
"filename(0)" -> "curr_dir(0)$0" [label = "Iter"]
"article_df(0)$0" -> "pd(0)" [label = "read_csv"]
"article_df(0)$0" -> "curr_dir(0)$0" [label = "read_csv"]
"article_df(0)$0" -> "filename(0)" [label = "read_csv"]
"all_headlines(0)$1" -> "all_headlines(0)$0" [label = "extend"]
"all_headlines(0)$1" -> "list(0)" [label = "extend"]
"all_headlines(0)$1" -> "article_df(0)$0" [label = "extend"]
"all_headlines(0)$2" -> "h(0)" [label = "eq"]
"all_headlines(0)$2" -> "h(0)" [label = "eq"]
"all_headlines(0)$2" -> "all_headlines(0)$2" [label = "eq"]
"all_headlines(0)$2" -> "h(0)" [label = "eq"]
"all_headlines(0)$2" -> "Unknown(0)" [label = "eq"]
"len[0]" -> "all_headlines(0)$2" [label = "len"]
"txt(1)" -> "clean_text[0]" [label = "_argToVar"]
"txt(1)$0" -> "(1)" [label = "lower"]
"txt(1)$0" -> "v(1)" [label = "lower"]
"txt(1)$0" -> "v(1)" [label = "lower"]
"txt(1)$0" -> "txt(1)$0" [label = "lower"]
"txt(1)$0" -> "v(1)" [label = "lower"]
"txt(1)$0" -> "string(1)" [label = "lower"]
"txt(1)$1" -> "txt(1)$0" [label = "decode"]
"txt(1)$1" -> "utf8(1)" [label = "decode"]
"txt(1)$1" -> "ascii(1)" [label = "decode"]
"txt(1)$1" -> "ignore(1)" [label = "decode"]
"corpus(0)$0" -> "clean_text(0)" [label = "eq"]
"corpus(0)$0" -> "x(0)" [label = "eq"]
"corpus(0)$0" -> "x(0)" [label = "eq"]
"corpus(0)$0" -> "all_headlines(0)$2" [label = "eq"]
"corpus(2)" -> "get_sequence_of_tokens[0]" [label = "_argToVar"]
"tokenizer(2)$0" -> "tokenizer(2)" [label = "fit_on_texts"]
"tokenizer(2)$0" -> "corpus(2)" [label = "fit_on_texts"]
"total_words(2)$0" -> "len(2)" [label = "Add"]
"total_words(2)$0" -> "tokenizer(2)$0" [label = "Add"]
"total_words(2)$0" -> "1(2)" [label = "Add"]
"input_sequences(2)$0" -> "[](2)" [label = "eq"]
"line(2)" -> "corpus(2)" [label = "Iter"]
"token_list(2)$0" -> "tokenizer(2)$0" [label = "eq"]
"token_list(2)$0" -> "[<_ast.Name object at 0x10448c3d0>](2)" [label = "eq"]
"token_list(2)$0" -> "0(2)" [label = "eq"]
"i(2)" -> "range(2)" [label = "Iter"]
"i(2)" -> "1(2)" [label = "Iter"]
"i(2)" -> "len(2)" [label = "Iter"]
"i(2)" -> "token_list(2)$0" [label = "Iter"]
"n_gram_sequence(2)$0" -> "token_list(2)$0" [label = "eq"]
"n_gram_sequence(2)$0" -> "i(2)" [label = "eq"]
"n_gram_sequence(2)$0" -> "1(2)" [label = "eq"]
"input_sequences(2)$1" -> "input_sequences(2)$0" [label = "append"]
"input_sequences(2)$1" -> "n_gram_sequence(2)$0" [label = "append"]
"inp_sequences(0)$0" -> "corpus(0)$0" [label = "get_sequence_of_tokens"]
"total_words(0)$0" -> "corpus(0)$0" [label = "get_sequence_of_tokens"]
"input_sequences(3)" -> "generate_padded_sequences[0]" [label = "_argToVar"]
"max_sequence_len(3)$0" -> "len(3)" [label = "max"]
"max_sequence_len(3)$0" -> "x(3)" [label = "max"]
"max_sequence_len(3)$0" -> "x(3)" [label = "max"]
"max_sequence_len(3)$0" -> "input_sequences(3)" [label = "max"]
"input_sequences(3)$0" -> "np(3)" [label = "array"]
"input_sequences(3)$0" -> "pad_sequences(3)" [label = "array"]
"input_sequences(3)$0" -> "input_sequences(3)$0" [label = "array"]
"input_sequences(3)$0" -> "max_sequence_len(3)$0" [label = "array"]
"input_sequences(3)$0" -> "pre(3)" [label = "array"]
"predictors(3)$0" -> "input_sequences(3)$0" [label = "eq"]
"label(3)$0" -> "input_sequences(3)$0" [label = "eq"]
"predictors(3)$0" -> "1(3)" [label = "eq"]
"label(3)$0" -> "1(3)" [label = "eq"]
"predictors(3)$0" -> "input_sequences(3)$0" [label = "eq"]
"label(3)$0" -> "input_sequences(3)$0" [label = "eq"]
"predictors(3)$0" -> "1(3)" [label = "eq"]
"label(3)$0" -> "1(3)" [label = "eq"]
"label(3)$1" -> "ku(3)" [label = "to_categorical"]
"label(3)$1" -> "label(3)$1" [label = "to_categorical"]
"label(3)$1" -> "total_words(3)" [label = "to_categorical"]
"predictors(0)$0" -> "inp_sequences(0)$0" [label = "generate_padded_sequences"]
"label(0)$0" -> "inp_sequences(0)$0" [label = "generate_padded_sequences"]
"max_sequence_len(0)$0" -> "inp_sequences(0)$0" [label = "generate_padded_sequences"]
"max_sequence_len(4)" -> "create_model[0]" [label = "_argToVar"]
"total_words(4)" -> "create_model[1]" [label = "_argToVar"]
"input_len(4)$0" -> "max_sequence_len(4)" [label = "Sub"]
"input_len(4)$0" -> "1(4)" [label = "Sub"]
"model(4)$0" -> "model(4)" [label = "add"]
"model(4)$0" -> "Embedding(4)" [label = "add"]
"model(4)$0" -> "total_words(4)" [label = "add"]
"model(4)$0" -> "10(4)" [label = "add"]
"model(4)$0" -> "input_len(4)$0" [label = "add"]
"model(4)$1" -> "model(4)$0" [label = "add"]
"model(4)$1" -> "LSTM(4)" [label = "add"]
"model(4)$1" -> "100(4)" [label = "add"]
"model(4)$2" -> "model(4)$1" [label = "add"]
"model(4)$2" -> "Dropout(4)" [label = "add"]
"model(4)$2" -> "0.1(4)" [label = "add"]
"model(4)$3" -> "model(4)$2" [label = "add"]
"model(4)$3" -> "Dense(4)" [label = "add"]
"model(4)$3" -> "total_words(4)" [label = "add"]
"model(4)$3" -> "softmax(4)" [label = "add"]
"model(4)$4" -> "model(4)$3" [label = "compile"]
"model(0)$0" -> "max_sequence_len(0)$0" [label = "create_model"]
"model(0)$0" -> "total_words(0)$0" [label = "create_model"]
"model(0)$1" -> "model(0)$0" [label = "summary"]
"model(0)$2" -> "model(0)$1" [label = "fit"]
"model(0)$2" -> "predictors(0)$0" [label = "fit"]
"model(0)$2" -> "label(0)$0" [label = "fit"]
"seed_text(5)" -> "generate_text[0]" [label = "_argToVar"]
"next_words(5)" -> "generate_text[1]" [label = "_argToVar"]
"model(5)" -> "generate_text[2]" [label = "_argToVar"]
"max_sequence_len(5)" -> "generate_text[3]" [label = "_argToVar"]
"_(5)" -> "range(5)" [label = "Iter"]
"_(5)" -> "next_words(5)" [label = "Iter"]
"token_list(5)$0" -> "tokenizer(5)" [label = "eq"]
"token_list(5)$0" -> "[<_ast.Name object at 0x104295c10>](5)" [label = "eq"]
"token_list(5)$0" -> "0(5)" [label = "eq"]
"token_list(5)$1" -> "[<_ast.Name object at 0x1042959d0>](5)" [label = "pad_sequences"]
"token_list(5)$1" -> "max_sequence_len(5)" [label = "pad_sequences"]
"token_list(5)$1" -> "1(5)" [label = "pad_sequences"]
"token_list(5)$1" -> "pre(5)" [label = "pad_sequences"]
"predicted(5)$0" -> "model(5)" [label = "predict_classes"]
"predicted(5)$0" -> "token_list(5)$1" [label = "predict_classes"]
"predicted(5)$0" -> "0(5)" [label = "predict_classes"]
"output_word(5)$0" -> "(5)" [label = "eq"]
"word(5)" -> "tokenizer(5)" [label = "Iter"]
"index(5)" -> "tokenizer(5)" [label = "Iter"]
"output_word(5)$1" -> "word(5)" [label = "eq"]
"seed_text(5)$0" -> " (5)" [label = "Add"]
"seed_text(5)$0" -> "seed_text(5)" [label = "Add"]
"seed_text(5)$0" -> "output_word(5)$1" [label = "Add"]
"seed_text(5)$0" -> "seed_text(5)" [label = "Add"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "united states(0)" [label = "print"]
"print[2]" -> "5(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "preident trump(0)" [label = "print"]
"print[2]" -> "4(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "donald trump(0)" [label = "print"]
"print[2]" -> "4(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "india and china(0)" [label = "print"]
"print[2]" -> "4(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "new york(0)" [label = "print"]
"print[2]" -> "4(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
"print[0]" -> "generate_text(0)" [label = "print"]
"print[1]" -> "science and technology(0)" [label = "print"]
"print[2]" -> "5(0)" [label = "print"]
"print[3]" -> "model(0)$2" [label = "print"]
"print[4]" -> "max_sequence_len(0)$0" [label = "print"]
}