digraph { 
"train(0)$0" -> "pd(0)" [label = "read_csv"]
"train(0)$0" -> "../input/train.csv(0)" [label = "read_csv"]
"test(0)$0" -> "pd(0)" [label = "read_csv"]
"test(0)$0" -> "../input/test.csv(0)" [label = "read_csv"]
"train(0)$1" -> "train(0)$0" [label = "head"]
"list_classes(0)$0" -> "[<_ast.Str object at 0x10b01f110>, <_ast.Str object at 0x10b01fad0>, <_ast.Str object at 0x10b01fd50>, <_ast.Str object at 0x10b01f5d0>, <_ast.Str object at 0x10b01fa90>, <_ast.Str object at 0x10b01fb10>](0)" [label = "eq"]
"y(0)$0" -> "train(0)$1" [label = "eq"]
"y(0)$0" -> "list_classes(0)$0" [label = "eq"]
"list_sentences_train(0)$0" -> "train(0)$1" [label = "eq"]
"list_sentences_train(0)$0" -> "comment_text(0)" [label = "eq"]
"list_sentences_test(0)$0" -> "test(0)$0" [label = "eq"]
"list_sentences_test(0)$0" -> "comment_text(0)" [label = "eq"]
"max_features(0)$0" -> "20000(0)" [label = "eq"]
"tokenizer(0)$0" -> "max_features(0)$0" [label = "Tokenizer"]
"tokenizer(0)$1" -> "tokenizer(0)$0" [label = "fit_on_texts"]
"tokenizer(0)$1" -> "list(0)" [label = "fit_on_texts"]
"tokenizer(0)$1" -> "list_sentences_train(0)$0" [label = "fit_on_texts"]
"list_tokenized_train(0)$0" -> "tokenizer(0)$1" [label = "texts_to_sequences"]
"list_tokenized_train(0)$0" -> "list_sentences_train(0)$0" [label = "texts_to_sequences"]
"list_tokenized_test(0)$0" -> "tokenizer(0)$1" [label = "texts_to_sequences"]
"list_tokenized_test(0)$0" -> "list_sentences_test(0)$0" [label = "texts_to_sequences"]
"maxlen(0)$0" -> "200(0)" [label = "eq"]
"X_t(0)$0" -> "list_tokenized_train(0)$0" [label = "pad_sequences"]
"X_t(0)$0" -> "maxlen(0)$0" [label = "pad_sequences"]
"X_te(0)$0" -> "list_tokenized_test(0)$0" [label = "pad_sequences"]
"X_te(0)$0" -> "maxlen(0)$0" [label = "pad_sequences"]
"totalNumWords(0)$0" -> "len(0)" [label = "eq"]
"totalNumWords(0)$0" -> "one_comment(0)" [label = "eq"]
"totalNumWords(0)$0" -> "one_comment(0)" [label = "eq"]
"totalNumWords(0)$0" -> "list_tokenized_train(0)$0" [label = "eq"]
"plt(0)$0" -> "plt(0)" [label = "hist"]
"plt(0)$0" -> "totalNumWords(0)$0" [label = "hist"]
"plt(0)$1" -> "plt(0)$0" [label = "show"]
"inp(0)$0" -> "maxlen(0)$0" [label = "Input"]
"embed_size(0)$0" -> "128(0)" [label = "eq"]
"x(0)$0" -> "Embedding(0)" [label = "eq"]
"x(0)$0" -> "max_features(0)$0" [label = "eq"]
"x(0)$0" -> "embed_size(0)$0" [label = "eq"]
"x(0)$0" -> "inp(0)$0" [label = "eq"]
"x(0)$1" -> "LSTM(0)" [label = "eq"]
"x(0)$1" -> "60(0)" [label = "eq"]
"x(0)$1" -> "lstm_layer(0)" [label = "eq"]
"x(0)$1" -> "x(0)$1" [label = "eq"]
"x(0)$2" -> "GlobalMaxPool1D(0)" [label = "eq"]
"x(0)$2" -> "x(0)$2" [label = "eq"]
"x(0)$3" -> "Dropout(0)" [label = "eq"]
"x(0)$3" -> "0.1(0)" [label = "eq"]
"x(0)$3" -> "x(0)$3" [label = "eq"]
"x(0)$4" -> "Dense(0)" [label = "eq"]
"x(0)$4" -> "50(0)" [label = "eq"]
"x(0)$4" -> "relu(0)" [label = "eq"]
"x(0)$4" -> "x(0)$4" [label = "eq"]
"x(0)$5" -> "Dropout(0)" [label = "eq"]
"x(0)$5" -> "0.1(0)" [label = "eq"]
"x(0)$5" -> "x(0)$5" [label = "eq"]
"x(0)$6" -> "Dense(0)" [label = "eq"]
"x(0)$6" -> "6(0)" [label = "eq"]
"x(0)$6" -> "sigmoid(0)" [label = "eq"]
"x(0)$6" -> "x(0)$6" [label = "eq"]
"model(0)$0" -> "inp(0)$0" [label = "Model"]
"model(0)$0" -> "x(0)$6" [label = "Model"]
"model(0)$1" -> "model(0)$0" [label = "compile"]
"batch_size(0)$0" -> "32(0)" [label = "eq"]
"epochs(0)$0" -> "2(0)" [label = "eq"]
"model(0)$2" -> "model(0)$1" [label = "fit"]
"model(0)$2" -> "X_t(0)$0" [label = "fit"]
"model(0)$2" -> "y(0)$0" [label = "fit"]
"model(0)$3" -> "model(0)$2" [label = "summary"]
"get_3rd_layer_output(0)$0" -> "K(0)" [label = "function"]
"get_3rd_layer_output(0)$0" -> "[<_ast.Attribute object at 0x10b02f990>](0)" [label = "function"]
"get_3rd_layer_output(0)$0" -> "[<_ast.Attribute object at 0x10b02f750>](0)" [label = "function"]
"layer_output(0)$0" -> "get_3rd_layer_output(0)$0" [label = "eq"]
"layer_output(0)$0" -> "[<_ast.Subscript object at 0x10b02fb50>](0)" [label = "eq"]
"layer_output(0)$0" -> "0(0)" [label = "eq"]
}