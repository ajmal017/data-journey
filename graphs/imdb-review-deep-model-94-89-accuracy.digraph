digraph { 
"warnings(0)$0" -> "warnings(0)" [label = "filterwarnings"]
"warnings(0)$0" -> "ignore(0)" [label = "filterwarnings"]
"reload[0]" -> "sys(0)" [label = "reload"]
"sys(0)$0" -> "sys(0)" [label = "setdefaultencoding"]
"sys(0)$0" -> "utf-8(0)" [label = "setdefaultencoding"]
"df1(0)$0" -> "pd(0)" [label = "read_csv"]
"df1(0)$0" -> "../input/word2vec-nlp-tutorial/labeledTrainData.tsv(0)" [label = "read_csv"]
"df1(0)$0" -> "	(0)" [label = "read_csv"]
"df1(0)$1" -> "df1(0)$0" [label = "drop"]
"df1(0)$1" -> "[<_ast.Str object at 0x10a3b3650>](0)" [label = "drop"]
"df1(0)$1" -> "1(0)" [label = "drop"]
"df1(0)$2" -> "df1(0)$1" [label = "head"]
"df2(0)$0" -> "pd(0)" [label = "read_csv"]
"df2(0)$0" -> "../input/imdb-review-dataset/imdb_master.csv(0)" [label = "read_csv"]
"df2(0)$0" -> "latin-1(0)" [label = "read_csv"]
"df2(0)$1" -> "df2(0)$0" [label = "head"]
"df2(0)$2" -> "df2(0)$1" [label = "drop"]
"df2(0)$2" -> "[<_ast.Str object at 0x10a3b3ad0>, <_ast.Str object at 0x10a3b3e50>, <_ast.Str object at 0x10a3b3710>](0)" [label = "drop"]
"df2(0)$2" -> "1(0)" [label = "drop"]
"df2(0)$3" -> "df2(0)$2" [label = "eq"]
"df2(0)$3" -> "[<_ast.Str object at 0x10a3b37d0>, <_ast.Str object at 0x10a3b3a50>](0)" [label = "eq"]
"df2(0)$4" -> "df2(0)$3" [label = "head"]
"df2(0)$5" -> "df2(0)$4" [label = "eq"]
"df2(0)$5" -> "df2(0)$5" [label = "eq"]
"df2(0)$5" -> "unsup(0)" [label = "eq"]
"df2(0)$6" -> "df2(0)$5" [label = "map"]
"df2(0)$6" -> "df2(0)$5" [label = "map"]
"df2(0)$6" -> "sentiment(0)" [label = "map"]
"df2(0)$6" -> "pos(0)" [label = "map"]
"df2(0)$6" -> "neg(0)" [label = "map"]
"df2(0)$6" -> "1(0)" [label = "map"]
"df2(0)$6" -> "0(0)" [label = "map"]
"df2(0)$7" -> "df2(0)$6" [label = "head"]
"df(0)$0" -> "pd(0)" [label = "reset_index"]
"df(0)$0" -> "[<_ast.Name object at 0x10b019f10>, <_ast.Name object at 0x10b019ad0>](0)" [label = "reset_index"]
"df(0)$1" -> "df(0)$0" [label = "head"]
"stop_words(0)$0" -> "stopwords(0)" [label = "set"]
"stop_words(0)$0" -> "english(0)" [label = "set"]
"text(1)" -> "clean_text[0]" [label = "_argToVar"]
"text(1)$0" -> "re(1)" [label = "sub"]
"text(1)$0" -> "[^\w\s](1)" [label = "sub"]
"text(1)$0" -> "(1)" [label = "sub"]
"text(1)$0" -> "text(1)$0" [label = "sub"]
"text(1)$0" -> "re(1)" [label = "sub"]
"text(1)$1" -> "text(1)$0" [label = "lower"]
"text(1)$2" -> "lemmatizer(1)" [label = "eq"]
"text(1)$2" -> "token(1)" [label = "eq"]
"text(1)$2" -> "token(1)" [label = "eq"]
"text(1)$2" -> "text(1)$2" [label = "eq"]
"text(1)$2" -> " (1)" [label = "eq"]
"text(1)$3" -> "lemmatizer(1)" [label = "eq"]
"text(1)$3" -> "token(1)" [label = "eq"]
"text(1)$3" -> "v(1)" [label = "eq"]
"text(1)$3" -> "token(1)" [label = "eq"]
"text(1)$3" -> "text(1)$3" [label = "eq"]
"text(1)$4" -> "word(1)" [label = "eq"]
"text(1)$4" -> "word(1)" [label = "eq"]
"text(1)$4" -> "text(1)$4" [label = "eq"]
"text(1)$4" -> "word(1)" [label = "eq"]
"text(1)$4" -> "stop_words(1)" [label = "eq"]
"text(1)$5" -> " (1)" [label = "join"]
"text(1)$5" -> "text(1)$5" [label = "join"]
"df(0)$2" -> "df(0)$1" [label = "apply"]
"df(0)$2" -> "df(0)$1" [label = "apply"]
"df(0)$2" -> "clean_text(0)" [label = "apply"]
"df(0)$2" -> "x(0)" [label = "apply"]
"df(0)$3" -> "df(0)$2" [label = "head"]
"df(0)$4" -> "df(0)$3" [label = "mean"]
"max_features(0)$0" -> "6000(0)" [label = "eq"]
"tokenizer(0)$0" -> "max_features(0)$0" [label = "Tokenizer"]
"tokenizer(0)$1" -> "tokenizer(0)$0" [label = "fit_on_texts"]
"tokenizer(0)$1" -> "df(0)$4" [label = "fit_on_texts"]
"tokenizer(0)$1" -> "Processed_Reviews(0)" [label = "fit_on_texts"]
"list_tokenized_train(0)$0" -> "tokenizer(0)$1" [label = "texts_to_sequences"]
"list_tokenized_train(0)$0" -> "df(0)$4" [label = "texts_to_sequences"]
"list_tokenized_train(0)$0" -> "Processed_Reviews(0)" [label = "texts_to_sequences"]
"maxlen(0)$0" -> "130(0)" [label = "eq"]
"X_t(0)$0" -> "list_tokenized_train(0)$0" [label = "pad_sequences"]
"X_t(0)$0" -> "maxlen(0)$0" [label = "pad_sequences"]
"y(0)$0" -> "df(0)$4" [label = "eq"]
"y(0)$0" -> "sentiment(0)" [label = "eq"]
"embed_size(0)$0" -> "128(0)" [label = "eq"]
"model(0)$0" -> "model(0)" [label = "add"]
"model(0)$0" -> "Embedding(0)" [label = "add"]
"model(0)$0" -> "max_features(0)$0" [label = "add"]
"model(0)$0" -> "embed_size(0)$0" [label = "add"]
"model(0)$1" -> "model(0)$0" [label = "add"]
"model(0)$1" -> "Bidirectional(0)" [label = "add"]
"model(0)$1" -> "LSTM(0)" [label = "add"]
"model(0)$1" -> "32(0)" [label = "add"]
"model(0)$2" -> "model(0)$1" [label = "add"]
"model(0)$2" -> "GlobalMaxPool1D(0)" [label = "add"]
"model(0)$3" -> "model(0)$2" [label = "add"]
"model(0)$3" -> "Dense(0)" [label = "add"]
"model(0)$3" -> "20(0)" [label = "add"]
"model(0)$3" -> "relu(0)" [label = "add"]
"model(0)$4" -> "model(0)$3" [label = "add"]
"model(0)$4" -> "Dropout(0)" [label = "add"]
"model(0)$4" -> "0.05(0)" [label = "add"]
"model(0)$5" -> "model(0)$4" [label = "add"]
"model(0)$5" -> "Dense(0)" [label = "add"]
"model(0)$5" -> "1(0)" [label = "add"]
"model(0)$5" -> "sigmoid(0)" [label = "add"]
"model(0)$6" -> "model(0)$5" [label = "compile"]
"batch_size(0)$0" -> "100(0)" [label = "eq"]
"epochs(0)$0" -> "3(0)" [label = "eq"]
"model(0)$7" -> "model(0)$6" [label = "fit"]
"model(0)$7" -> "X_t(0)$0" [label = "fit"]
"model(0)$7" -> "y(0)$0" [label = "fit"]
"df_test(0)$0" -> "pd(0)" [label = "read_csv"]
"df_test(0)$0" -> "../input/word2vec-nlp-tutorial/testData.tsv(0)" [label = "read_csv"]
"df_test(0)$0" -> "0(0)" [label = "read_csv"]
"df_test(0)$0" -> "	(0)" [label = "read_csv"]
"df_test(0)$0" -> "3(0)" [label = "read_csv"]
"df_test(0)$1" -> "df_test(0)$0" [label = "head"]
"df_test(0)$2" -> "df_test(0)$1" [label = "apply"]
"df_test(0)$2" -> "df_test(0)$1" [label = "apply"]
"df_test(0)$2" -> "clean_text(0)" [label = "apply"]
"df_test(0)$2" -> "x(0)" [label = "apply"]
"df_test(0)$3" -> "df_test(0)$2" [label = "map"]
"df_test(0)$3" -> "df_test(0)$2" [label = "map"]
"df_test(0)$3" -> "id(0)" [label = "map"]
"df_test(0)$3" -> "int(0)" [label = "map"]
"df_test(0)$3" -> "x(0)" [label = "map"]
"df_test(0)$3" -> ""(0)" [label = "map"]
"df_test(0)$3" -> "_(0)" [label = "map"]
"df_test(0)$3" -> "1(0)" [label = "map"]
"df_test(0)$3" -> "5(0)" [label = "map"]
"df_test(0)$3" -> "1(0)" [label = "map"]
"df_test(0)$3" -> "0(0)" [label = "map"]
"y_test(0)$0" -> "df_test(0)$3" [label = "eq"]
"y_test(0)$0" -> "sentiment(0)" [label = "eq"]
"list_sentences_test(0)$0" -> "df_test(0)$3" [label = "eq"]
"list_sentences_test(0)$0" -> "review(0)" [label = "eq"]
"list_tokenized_test(0)$0" -> "tokenizer(0)$1" [label = "texts_to_sequences"]
"list_tokenized_test(0)$0" -> "list_sentences_test(0)$0" [label = "texts_to_sequences"]
"X_te(0)$0" -> "list_tokenized_test(0)$0" [label = "pad_sequences"]
"X_te(0)$0" -> "maxlen(0)$0" [label = "pad_sequences"]
"prediction(0)$0" -> "model(0)$7" [label = "predict"]
"prediction(0)$0" -> "X_te(0)$0" [label = "predict"]
"y_pred(0)$0" -> "prediction(0)$0" [label = "eq"]
"y_pred(0)$0" -> "0.5(0)" [label = "eq"]
"print[0]" -> "F1-score: {0}(0)" [label = "print"]
"print[1]" -> "f1_score(0)" [label = "print"]
"print[2]" -> "y_pred(0)$0" [label = "print"]
"print[3]" -> "y_test(0)$0" [label = "print"]
"print[0]" -> "Confusion matrix:(0)" [label = "print"]
"confusion_matrix[0]" -> "y_pred(0)$0" [label = "confusion_matrix"]
"confusion_matrix[1]" -> "y_test(0)$0" [label = "confusion_matrix"]
}